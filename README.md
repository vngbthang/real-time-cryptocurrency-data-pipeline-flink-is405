# ðŸ’° Real-Time Cryptocurrency Analytics Pipeline

## ðŸ†• **SPARK vs FLINK COMPARISON PROJECT**

> **Má»¥c tiÃªu má»Ÿ rá»™ng (IS405)**: So sÃ¡nh hiá»‡u suáº¥t giá»¯a **Apache Spark Streaming** vÃ  **Apache Flink** trong xá»­ lÃ½ dá»¯ liá»‡u real-time.

### ðŸ”¥ Äiá»ƒm ná»•i báº­t cá»§a pháº§n má»Ÿ rá»™ng:
- âœ… **Song song 2 engine**: CÃ¹ng xá»­ lÃ½ 1 nguá»“n dá»¯ liá»‡u Kafka
- âœ… **So sÃ¡nh thá»±c nghiá»‡m**: Latency, Throughput, Resource Usage
- âœ… **Dashboard riÃªng**: Spark UI (8081) vs Flink Dashboard (8082)
- âœ… **Tá»± Ä‘á»™ng hÃ³a**: Script demo PowerShell Ä‘á»ƒ quan sÃ¡t real-time

ðŸ“– **[Xem chi tiáº¿t so sÃ¡nh táº¡i Ä‘Ã¢y](docs/SPARK_VS_FLINK_COMPARISON.md)**

---

## ðŸ“‹ Má»¥c tiÃªu (Objective)

Project nÃ y xÃ¢y dá»±ng má»™t **Real-Time ETL Pipeline** hoÃ n chá»‰nh theo kiáº¿n trÃºc **Medallion** (Bronze-Silver-Gold) Ä‘á»ƒ xá»­ lÃ½ dá»¯ liá»‡u cryptocurrency tá»« Coinbase API, cung cáº¥p analytics vÃ  insights theo thá»i gian thá»±c cho 5 loáº¡i cryptocurrency: **BTC, ETH, SOL, ADA, DOGE**.

**Váº¥n Ä‘á» giáº£i quyáº¿t:**

- **Real-time ingestion**: Thu tháº­p dá»¯ liá»‡u giÃ¡ vÃ  khá»‘i lÆ°á»£ng giao dá»‹ch tá»« Coinbase API má»—i 10 giÃ¢y.
- **Stream processing**: Xá»­ lÃ½ dá»¯ liá»‡u real-time vá»›i Spark Structured Streaming **+ Apache Flink (má»Ÿ rá»™ng)**.
- **Data aggregation**: Táº¡o metrics theo cá»­a sá»• thá»i gian (10 phÃºt, 1 giá») cho phÃ¢n tÃ­ch.
- **Orchestration**: Tá»± Ä‘á»™ng hÃ³a pipeline vá»›i Apache Airflow.
- **Analytics ready**: Cung cáº¥p dá»¯ liá»‡u sáºµn sÃ ng cho BI tools (Grafana, pgAdmin, REST API).

## ðŸ—ï¸ Kiáº¿n trÃºc (Architecture)

![Architecture](docs/images/architecture.png)

**Kiáº¿n trÃºc Medallion**: Bronze (Kafka) â†’ Silver (Raw Data) â†’ Gold (Aggregated Metrics)

## ðŸ› ï¸ CÃ´ng nghá»‡ sá»­ dá»¥ng (Tech Stack)

| Component             | Technology                      | Version   |
|-----------------------|---------------------------------|-----------|
| **Message Broker**    | Apache Kafka                    | 7.3.0     |
| **Stream Processing** | Apache Spark Structured Streaming | 3.5.0     |
| **Stream Processing (NEW)** | **Apache Flink**           | **1.18.0** |
| **Database**          | PostgreSQL                      | 14        |
| **Orchestration**     | Apache Airflow                  | 2.8.1     |
| **Data Source**       | Coinbase API                    | v2        |
| **Producer**          | Python + kafka-python           | 3.11 / 2.0.2 |
| **Container Platform**| Docker + Docker Compose         | Latest    |
| **BI Visualization**  | Grafana (optional)              | Latest    |
| **API Framework**     | FastAPI (optional)              | Latest    |

> **ðŸ†• Apache Flink** Ä‘Æ°á»£c thÃªm vÃ o Ä‘á»ƒ so sÃ¡nh hiá»‡u suáº¥t vá»›i Spark Streaming

## ðŸ“Š Cáº¥u trÃºc Dá»¯ liá»‡u (Schema)

### Infrastructure Layout:

![ArchitectureLayout](docs/images/infralayout.png) 

### Data Schema:

![Schema](docs/images/graph.png) 

### Tracked Cryptocurrencies:

```python
CRYPTO_PAIRS = [
    'BTC-USD',  # Bitcoin
    'ETH-USD',  # Ethereum
    'SOL-USD',  # Solana
    'ADA-USD',  # Cardano
    'DOGE-USD'  # Dogecoin
]
```

## ðŸš€ CÃ¡ch thiáº¿t láº­p vÃ  cháº¡y (Setup & Run)

### Prerequisites:

- Docker Desktop (Windows)
- Docker Compose
- Git
- PowerShell
- Minimum 8GB RAM, 20GB disk space

### BÆ°á»›c 1: Clone Repository

```powershell
git clone https://github.com/vngbthang/real-time-cryptocurrency-data-pipeline.git
cd real-time-cryptocurrency-data-pipeline
```

### BÆ°á»›c 2: Khá»Ÿi Ä‘á»™ng Infrastructure

```powershell
# Start all Docker containers
docker-compose up -d

# Verify all containers are running (should see 11 containers)
docker ps
```

**LÆ°u Ã½:** Database schema vÃ  Kafka topics sáº½ Ä‘Æ°á»£c táº¡o tá»± Ä‘á»™ng khi containers khá»Ÿi Ä‘á»™ng láº§n Ä‘áº§u:
- âœ… PostgreSQL tables: `crypto_prices_realtime`, `gold_hourly_metrics`, `gold_10min_metrics` (via `init-db.sql`)
- âœ… Kafka topic: `crypto_prices` (via `kafka-init` container)
- âœ… Producer: Tá»± Ä‘á»™ng start vÃ  báº¯t Ä‘áº§u gá»­i dá»¯ liá»‡u

### BÆ°á»›c 3: Trigger Spark Streaming Job

```powershell
# Open Airflow UI
Start-Process "http://localhost:8080"

# Login: admin / admin
# Navigate to DAGs -> Find "crypto_streaming_pipeline"
# Click "Trigger DAG" (play icon)
```

### BÆ°á»›c 4: Enable Gold Layer Aggregation

Airflow UI, unpause cÃ¡c DAGs:
- `gold_hourly_aggregation`
- `gold_10min_aggregation`

---

## ðŸ†• **DEMO SO SÃNH SPARK vs FLINK**

### Quick Start vá»›i Demo Script:

```powershell
# Cháº¡y script demo tá»± Ä‘á»™ng
.\demo.ps1
```

Script sáº½:
1. âœ… Khá»Ÿi Ä‘á»™ng táº¥t cáº£ services (Kafka, Spark, Flink, PostgreSQL)
2. âœ… Chá» dá»¯ liá»‡u Ä‘Æ°á»£c xá»­ lÃ½
3. âœ… Hiá»ƒn thá»‹ menu tÆ°Æ¡ng tÃ¡c Ä‘á»ƒ so sÃ¡nh:
   - Tá»•ng quan dá»¯ liá»‡u
   - So sÃ¡nh Ä‘á»™ trá»… (Latency)
   - So sÃ¡nh thÃ´ng lÆ°á»£ng (Throughput)
   - Xem logs vÃ  dashboards

### Hoáº·c cháº¡y thá»§ cÃ´ng:

```powershell
# 1. Khá»Ÿi Ä‘á»™ng há»‡ thá»‘ng
docker-compose up -d

# 2. Kiá»ƒm tra Flink Ä‘ang cháº¡y
docker logs flink-crypto-processor

# 3. Truy cáº­p Dashboards
Start-Process "http://localhost:8082"  # Flink Dashboard
Start-Process "http://localhost:8081"  # Spark UI

# 4. So sÃ¡nh dá»¯ liá»‡u
docker exec -it postgres-db psql -U user -d crypto_data -f /sql/comparison_queries.sql
```

### ðŸ“Š Web Dashboards:
- **Spark Master UI**: http://localhost:8081
- **Flink Dashboard**: http://localhost:8082
- **Airflow UI**: http://localhost:8080 (admin/admin)

---

### BÆ°á»›c 5: Kiá»ƒm tra dá»¯ liá»‡u

```powershell
# Check Silver layer
docker exec -it postgres-db psql -U user -d crypto_data -c "SELECT symbol, price, processed_at FROM crypto_prices_realtime ORDER BY processed_at DESC LIMIT 5;"

# Check Gold Hourly Metrics
docker exec -it postgres-db psql -U user -d crypto_data -c "SELECT symbol, ROUND(avg_price::numeric, 2) as avg_price, ROUND(price_change_percent::numeric, 2) as change_pct FROM gold_hourly_metrics ORDER BY hour_timestamp DESC LIMIT 5;"

# Check Gold 10-Minute Metrics
docker exec -it postgres-db psql -U user -d crypto_data -c "SELECT symbol, ROUND(avg_price::numeric, 2) as avg, ROUND(price_volatility::numeric, 2) as volatility FROM gold_10min_metrics ORDER BY window_start DESC LIMIT 5;"
```

## ðŸ“ˆ Káº¿t quáº£ (Final Output)

### Silver Layer Table: `crypto_prices_realtime`
- **Má»¥c Ä‘Ã­ch:** Raw structured data from Spark streaming.
- **Use cases:** Real-time price monitoring, data quality checks, raw data for ad-hoc analysis.

### Gold Layer Table 1: `gold_hourly_metrics`
- **Má»¥c Ä‘Ã­ch:** Hourly aggregated analytics.
- **Use cases:** Historical trend analysis, day-over-day comparisons, hourly performance reports.

### Gold Layer Table 2: `gold_10min_metrics`
- **Má»¥c Ä‘Ã­ch:** Near real-time analytics with 10-minute windows.
- **Use cases:** Real-time volatility monitoring, short-term trading signals, anomaly detection.

## ðŸ“ Cáº¥u trÃºc Project

```
real-time-cryptocurrency-data-pipeline/
â”‚
â”œâ”€â”€ docker-compose.yml              # Infrastructure orchestration (11 containers)
â”œâ”€â”€ Dockerfile.producer             # Containerized producer build file
â”œâ”€â”€ requirements.txt                # Python dependencies
â”œâ”€â”€ README.md                       # This file
â”‚
â”œâ”€â”€ coinbase_producer.py            # Multi-coin data producer
â”‚
â”œâ”€â”€ init-kafka.sh                   # Auto-create Kafka topics on startup
â”œâ”€â”€ init-airflow.sh                 # Auto-trigger Airflow DAGs (optional)
â”œâ”€â”€ init-db.sql                     # Auto-create database schema on startup
â”‚
â”œâ”€â”€ spark-apps/
â”‚   â””â”€â”€ spark_stream_processor.py   # Spark Structured Streaming job
â”‚
â”œâ”€â”€ dags/                           # Airflow orchestration
â”‚   â”œâ”€â”€ auto_startup_pipeline.py    # Auto-startup orchestration
â”‚   â”œâ”€â”€ crypto_producer.py          # Producer DAG (alternative)
â”‚   â”œâ”€â”€ submit_spark_stream.py      # Main streaming pipeline DAG
â”‚   â”œâ”€â”€ gold_aggregation.py         # Hourly metrics aggregation
â”‚   â””â”€â”€ gold_10min_aggregation.py   # 10-minute metrics aggregation
â”‚
â””â”€â”€ logs/                           # Airflow logs directory
```

## ðŸŽ¯ Key Features

- âœ… **Real-Time Processing**: 10-second polling and 15-second micro-batching.
- âœ… **Multi-Cryptocurrency Support**: Tracks 5 major coins, easily extensible.
- âœ… **Medallion Architecture**: Bronze (Kafka), Silver (PostgreSQL), and Gold (PostgreSQL) layers.
- âœ… **Data Quality & Reliability**: Spark checkpointing, producer retry logic, and data retention policies.
- âœ… **Orchestration & Monitoring**: Fully automated with Airflow and monitored via Spark UI and Airflow UI.
- âœ… **Scalability**: Designed for horizontal and vertical scaling.
- âœ… **Analytics Ready**: Pre-aggregated metrics in the Gold layer for fast BI queries.

## ðŸ“š Deployment Guide (HÆ°á»›ng dáº«n Triá»ƒn khai Chi tiáº¿t)

### ðŸ” Monitoring & Verification (GiÃ¡m sÃ¡t & Kiá»ƒm tra)

#### Initialization Scripts (Scripts Khá»Ÿi táº¡o Tá»± Ä‘á»™ng)

Project sá»­ dá»¥ng cÃ¡c script tá»± Ä‘á»™ng Ä‘á»ƒ khá»Ÿi táº¡o mÃ´i trÆ°á»ng:

**1. `init-kafka.sh` (Kafka Initialization)**
- **Chá»©c nÄƒng**: Tá»± Ä‘á»™ng táº¡o Kafka topic `crypto_prices` khi Kafka container khá»Ÿi Ä‘á»™ng
- **Chi tiáº¿t**:
  - Äá»£i 30 giÃ¢y cho Kafka sáºµn sÃ ng
  - Táº¡o topic vá»›i 3 partitions, replication factor = 1
  - Liá»‡t kÃª táº¥t cáº£ topics Ä‘á»ƒ verify
- **Container**: `kafka-init` trong docker-compose.yml
- **Log kiá»ƒm tra**:
```powershell
docker logs kafka-init
```

**2. `init-airflow.sh` (Airflow Initialization - Optional)**
- **Chá»©c nÄƒng**: Tá»± Ä‘á»™ng trigger DAGs khi Airflow khá»Ÿi Ä‘á»™ng (náº¿u muá»‘n tá»± Ä‘á»™ng hÃ³a)
- **Chi tiáº¿t**:
  - Äá»£i 60 giÃ¢y cho Airflow webserver sáºµn sÃ ng
  - Unpause vÃ  trigger `crypto_streaming_pipeline`
  - Unpause `gold_hourly_aggregation` vÃ  `gold_10min_aggregation`
- **LÆ°u Ã½**: Script nÃ y chÆ°a Ä‘Æ°á»£c tÃ­ch há»£p vÃ o docker-compose (cháº¡y thá»§ cÃ´ng náº¿u cáº§n)
- **CÃ¡ch cháº¡y thá»§ cÃ´ng**:
```powershell
docker exec -it airflow-webserver bash /opt/airflow/init-airflow.sh
```

**3. `init-db.sql` (Database Schema Initialization)**
- **Chá»©c nÄƒng**: Tá»± Ä‘á»™ng táº¡o database schema khi PostgreSQL container khá»Ÿi Ä‘á»™ng láº§n Ä‘áº§u
- **Chi tiáº¿t**:
  - Táº¡o 3 báº£ng: `crypto_prices_realtime`, `gold_hourly_metrics`, `gold_10min_metrics`
  - Sá»­ dá»¥ng `CREATE TABLE IF NOT EXISTS` Ä‘á»ƒ trÃ¡nh lá»—i náº¿u cháº¡y láº¡i
- **Container**: Mounted vÃ o `postgres-db` táº¡i `/docker-entrypoint-initdb.d/`
- **Log kiá»ƒm tra**:
```powershell
docker logs postgres-db | Select-String -Pattern "init-db"
```

#### Kafka Topics
```powershell
# Liá»‡t kÃª táº¥t cáº£ topics
docker exec -it kafka kafka-topics --bootstrap-server localhost:9092 --list

# Kiá»ƒm tra messages trong topic crypto_prices
docker exec -it kafka kafka-console-consumer --bootstrap-server localhost:9092 --topic crypto_prices --from-beginning --max-messages 5
```

#### Spark Jobs
```powershell
# Truy cáº­p Spark Master UI
Start-Process "http://localhost:8081"

# Xem log Spark Master
docker logs spark-master --tail 50

# Xem log Spark Worker
docker logs spark-worker --tail 50
```

#### Airflow DAGs
```powershell
# Xem danh sÃ¡ch DAGs
docker exec -it airflow-webserver airflow dags list

# Xem task instances
docker exec -it airflow-webserver airflow tasks list crypto_streaming_pipeline
```

#### Database Queries
```powershell
# Äáº¿m sá»‘ records theo symbol
docker exec -it postgres-db psql -U user -d crypto_data -c "SELECT symbol, COUNT(*) FROM crypto_prices_realtime GROUP BY symbol;"

# Kiá»ƒm tra giÃ¡ má»›i nháº¥t
docker exec -it postgres-db psql -U user -d crypto_data -c "SELECT DISTINCT ON (symbol) symbol, price, processed_at FROM crypto_prices_realtime ORDER BY symbol, processed_at DESC;"

# PhÃ¢n tÃ­ch volume (khi cÃ³ dá»¯ liá»‡u)
docker exec -it postgres-db psql -U user -d crypto_data -c "SELECT symbol, AVG(volume_24h) as avg_volume FROM crypto_prices_realtime WHERE volume_24h IS NOT NULL GROUP BY symbol;"
```

### ðŸ› ï¸ Troubleshooting (Xá»­ lÃ½ sá»± cá»‘)

#### Producer Issues
**Váº¥n Ä‘á»**: Lá»—i "NoBrokersAvailable"
```powershell
# Kiá»ƒm tra Kafka cÃ³ cháº¡y khÃ´ng
docker logs kafka | Select-String -Pattern "started"

# Test káº¿t ná»‘i
Test-NetConnection localhost -Port 9093

# Náº¿u khÃ´ng Ä‘Æ°á»£c, restart Kafka
docker-compose restart kafka
Start-Sleep -Seconds 20
```

**Váº¥n Ä‘á»**: API rate limiting
- TÄƒng `POLL_INTERVAL_SECONDS` trong `coinbase_producer.py` (máº·c Ä‘á»‹nh: 10)

#### Spark Job Issues
**Váº¥n Ä‘á»**: Job khÃ´ng xá»­ lÃ½ dá»¯ liá»‡u
```powershell
# Kiá»ƒm tra log Spark
docker logs spark-master
docker logs spark-worker

# Kiá»ƒm tra káº¿t ná»‘i Kafka tá»« Spark
docker exec -it spark-master nc -zv kafka 9092
```

**Váº¥n Ä‘á»**: Checkpoint bá»‹ há»ng
```powershell
# XÃ³a checkpoints vÃ  restart
docker exec -it spark-master rm -rf /opt/spark/apps/checkpoints/*
```

#### Database Issues
**Váº¥n Ä‘á»**: Connection refused
```powershell
# Kiá»ƒm tra PostgreSQL Ä‘ang cháº¡y
docker exec -it postgres-db pg_isready

# Kiá»ƒm tra logs
docker logs postgres-db
```

**Váº¥n Ä‘á»**: Schema mismatch
```powershell
# Cháº¡y láº¡i migration script
docker cp sql/alter_tables_add_volume.sql postgres-db:/tmp/
docker exec -it postgres-db psql -U user -d crypto_data -f /tmp/alter_tables_add_volume.sql
```

### ðŸ“ˆ Next Steps (BÆ°á»›c tiáº¿p theo)

#### Monitoring Producer Container
```powershell
# Xem logs cá»§a producer
docker logs crypto-producer --tail 50 -f

# Kiá»ƒm tra producer Ä‘ang cháº¡y
docker exec crypto-producer ps aux

# Restart producer náº¿u cáº§n
docker-compose restart crypto-producer
```

#### ThÃªm Volume Data Thá»±c táº¿
Hiá»‡n táº¡i `volume_24h` Ä‘ang lÃ  `None` vÃ¬ Coinbase API v2 `/spot` endpoint khÃ´ng cung cáº¥p volume. Äá»ƒ thÃªm volume thá»±c:

**Option 1: Sá»­ dá»¥ng Coinbase Advanced Trade API**
```python
# Cáº§n xÃ¡c thá»±c
COINBASE_PRODUCT_API = 'https://api.coinbase.com/api/v3/brokerage/products/{pair}/ticker'
# Tráº£ vá»: price, volume_24h, price_percent_change_24h
```

**Option 2: Sá»­ dá»¥ng CoinGecko API (khÃ´ng cáº§n xÃ¡c thá»±c)**
```python
COINGECKO_API = 'https://api.coingecko.com/api/v3/simple/price'
# Parameters: ids=bitcoin,ethereum&vs_currencies=usd&include_24hr_vol=true
```

Cáº­p nháº­t `coinbase_producer.py` Ä‘á»ƒ láº¥y volume data tá»« CoinGecko hoáº·c Advanced Trade API.

---

## ðŸ”Œ BI Integration Guide (HÆ°á»›ng dáº«n TÃ­ch há»£p BI)

### ðŸ“Š Káº¿t ná»‘i Database

#### ThÃ´ng tin káº¿t ná»‘i PostgreSQL
```
Host: localhost
Port: 5432
Database: crypto_data
Username: user
Password: password
```

### Power BI / Tableau

#### BÆ°á»›c 1: Chá»n Data Source
- Má»Ÿ Power BI Desktop hoáº·c Tableau
- Chá»n "PostgreSQL" lÃ m data source
- Nháº­p thÃ´ng tin káº¿t ná»‘i á»Ÿ trÃªn

#### BÆ°á»›c 2: Select Tables
Chá»n cÃ¡c báº£ng:
- `crypto_prices_realtime` (Silver Layer) - Real-time data
- `gold_hourly_metrics` (Gold Layer) - Hourly analytics
- `gold_10min_metrics` (Gold Layer) - 10-minute analytics

#### BÆ°á»›c 3: Táº¡o Visualizations

**Dashboard 1: Real-Time Price Monitor**
```sql
SELECT 
    symbol,
    price,
    processed_at,
    LAG(price) OVER (PARTITION BY symbol ORDER BY processed_at) as prev_price,
    ROUND(((price - LAG(price) OVER (PARTITION BY symbol ORDER BY processed_at)) / 
           LAG(price) OVER (PARTITION BY symbol ORDER BY processed_at) * 100)::numeric, 2) as pct_change
FROM crypto_prices_realtime
WHERE processed_at >= NOW() - INTERVAL '1 hour'
ORDER BY processed_at DESC;
```

**Dashboard 2: Hourly Trend Analysis**
```sql
SELECT 
    hour_timestamp,
    symbol,
    avg_price,
    min_price,
    max_price,
    price_change_percent,
    total_volume
FROM gold_hourly_metrics
WHERE hour_timestamp >= NOW() - INTERVAL '24 hours'
ORDER BY hour_timestamp DESC;
```

**Dashboard 3: Volatility Monitor**
```sql
SELECT 
    window_start,
    symbol,
    avg_price,
    price_volatility,
    (max_price - min_price) as price_range,
    ROUND(((max_price - min_price) / avg_price * 100)::numeric, 2) as volatility_pct
FROM gold_10min_metrics
WHERE window_start >= NOW() - INTERVAL '2 hours'
ORDER BY window_start DESC;
```

### pgAdmin (Database Management)

#### Setup pgAdmin
```powershell
# Pull pgAdmin image
docker pull dpage/pgadmin4

# Run pgAdmin container
docker run -d `
  --name pgadmin `
  --network crypto-pipeline-net `
  -p 5050:80 `
  -e PGADMIN_DEFAULT_EMAIL=admin@admin.com `
  -e PGADMIN_DEFAULT_PASSWORD=admin `
  dpage/pgadmin4
```

#### Truy cáº­p pgAdmin
1. Má»Ÿ browser: http://localhost:5050
2. Login: `admin@admin.com` / `admin`
3. Add New Server:
   - Name: `Crypto Pipeline`
   - Host: `postgres-db`
   - Port: `5432`
   - Username: `user`
   - Password: `password`

### Grafana (Optional)

#### Setup Grafana
```powershell
# Pull Grafana image
docker pull grafana/grafana

# Run Grafana container
docker run -d `
  --name grafana `
  --network crypto-pipeline-net `
  -p 3000:3000 `
  grafana/grafana
```

#### Configure Grafana
1. Truy cáº­p: http://localhost:3000
2. Login: `admin` / `admin`
3. Add PostgreSQL Data Source:
   - Host: `postgres-db:5432`
   - Database: `crypto_data`
   - User: `user`
   - Password: `password`
   - SSL Mode: `disable`

#### Sample Grafana Queries

**Panel 1: Current Prices**
```sql
SELECT 
  processed_at as time,
  symbol as metric,
  price as value
FROM crypto_prices_realtime
WHERE $__timeFilter(processed_at)
ORDER BY processed_at;
```

**Panel 2: Hourly Average Prices**
```sql
SELECT 
  hour_timestamp as time,
  symbol as metric,
  avg_price as value
FROM gold_hourly_metrics
WHERE $__timeFilter(hour_timestamp)
ORDER BY hour_timestamp;
```

**Panel 3: Price Volatility**
```sql
SELECT 
  window_start as time,
  symbol as metric,
  price_volatility as value
FROM gold_10min_metrics
WHERE $__timeFilter(window_start)
ORDER BY window_start;
```

### REST API (FastAPI - Optional)

Táº¡o file `api/main.py`:
```python
from fastapi import FastAPI
import psycopg2
from psycopg2.extras import RealDictCursor

app = FastAPI()

DB_CONFIG = {
    "host": "localhost",
    "port": 5432,
    "database": "crypto_data",
    "user": "user",
    "password": "password"
}

@app.get("/api/prices/latest")
def get_latest_prices():
    conn = psycopg2.connect(**DB_CONFIG, cursor_factory=RealDictCursor)
    cur = conn.cursor()
    cur.execute("""
        SELECT DISTINCT ON (symbol) 
            symbol, price, processed_at
        FROM crypto_prices_realtime
        ORDER BY symbol, processed_at DESC;
    """)
    results = cur.fetchall()
    cur.close()
    conn.close()
    return results

@app.get("/api/metrics/hourly/{symbol}")
def get_hourly_metrics(symbol: str, hours: int = 24):
    conn = psycopg2.connect(**DB_CONFIG, cursor_factory=RealDictCursor)
    cur = conn.cursor()
    cur.execute("""
        SELECT * FROM gold_hourly_metrics
        WHERE symbol = %s 
        AND hour_timestamp >= NOW() - INTERVAL '%s hours'
        ORDER BY hour_timestamp DESC;
    """, (symbol, hours))
    results = cur.fetchall()
    cur.close()
    conn.close()
    return results
```

Cháº¡y API:
```powershell
pip install fastapi uvicorn psycopg2-binary
uvicorn api.main:app --reload --port 8000
```

Truy cáº­p API docs: http://localhost:8000/docs