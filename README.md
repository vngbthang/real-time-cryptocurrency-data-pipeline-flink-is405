# Apache Flink: BÃ¡o cÃ¡o vÃ  Demo xá»­ lÃ½ dá»¯ liá»‡u lá»›n (So sÃ¡nh vá»›i Apache Spark)

> BÃ¡o cÃ¡o mÃ´n há»c IS405 â€“ Há»‡ thá»‘ng minh há»a Flink káº¿t há»£p/so sÃ¡nh vá»›i Spark Ä‘á»ƒ xá»­ lÃ½ dá»¯ liá»‡u streaming quy mÃ´ lá»›n trong thá»i gian thá»±c.

[![Docker](https://img.shields.io/badge/Docker-Ready-blue)](https://www.docker.com/)
[![Flink](https://img.shields.io/badge/Flink-1.18.0-red)](https://flink.apache.org/)
[![Spark](https://img.shields.io/badge/Spark-3.5.0-orange)](https://spark.apache.org/)
[![Kafka](https://img.shields.io/badge/Kafka-7.3.0-black)](https://kafka.apache.org/)
[![Airflow](https://img.shields.io/badge/Airflow-2.8.1-lightgrey)](https://airflow.apache.org/)
[![PostgreSQL](https://img.shields.io/badge/PostgreSQL-14-blue)](https://www.postgresql.org/)

---

## ğŸ“‹ Má»¥c lá»¥c

1. ThÃ´ng tin chung vá» Apache Flink
2. Äáº·c trÆ°ng, Æ¯u/NhÆ°á»£c Ä‘iá»ƒm
3. Case study (á»©ng dá»¥ng thá»±c táº¿)
4. Cáº¥u trÃºc & Kiáº¿n trÃºc há»‡ thá»‘ng demo
5. CÃ i Ä‘áº·t, káº¿t ná»‘i vÃ  cÃ¡ch cháº¡y há»‡ thá»‘ng
6. So sÃ¡nh Flink vá»›i Spark (kiáº¿n trÃºc & thá»±c nghiá»‡m)
7. Äiá»u chá»‰nh tham sá»‘ (tuning)
8. Minh há»a váº­n hÃ nh xá»­ lÃ½ dá»¯ liá»‡u lá»›n
9. Káº¿t quáº£ thá»±c nghiá»‡m (sá»‘ liá»‡u Ä‘o)
10. Káº¿t luáº­n

---

## 1. ThÃ´ng tin chung vá» Apache Flink

**Apache Flink** lÃ  framework vÃ  engine phÃ¢n tÃ¡n cho xá»­ lÃ½ dá»¯ liá»‡u **streaming** vÃ  **batch**, ná»•i báº­t vá»›i:
- Xá»­ lÃ½ sá»± kiá»‡n theo thá»i gian thá»±c, há»— trá»£ **Event Time** vÃ  **Watermarks**.
- **Exactly-once semantics**, stateful streaming vÃ  checkpointing tin cáº­y.
- Kháº£ nÄƒng **scale-out** máº¡nh máº½ vá»›i TaskManagers/JobManager.
- Há»— trá»£ API phong phÃº: DataStream API, Table/SQL, CEP.

Use-cases Ä‘iá»ƒn hÃ¬nh: phÃ¡t hiá»‡n gian láº­n, giÃ¡m sÃ¡t há»‡ thá»‘ng, phÃ¢n tÃ­ch log real-time, recommendation.

---

## 2. Äáº·c trÆ°ng, Æ¯u/NhÆ°á»£c Ä‘iá»ƒm

**Æ¯u Ä‘iá»ƒm**
- Äá»™ trá»… tháº¥p (low latency) cho streaming thá»±c sá»±.
- Event-time chuáº©n, xá»­ lÃ½ out-of-order vá»›i watermarks.
- Exactly-once, state backend hiá»‡u quáº£, checkpoint/restore.
- Tá»‘i Æ°u cho long-running jobs, á»•n Ä‘á»‹nh khi cháº¡y 24/7.

**NhÆ°á»£c Ä‘iá»ƒm**
- ÄÆ°á»ng cong há»c táº­p (learning curve) cao hÆ¡n.
- Há»‡ sinh thÃ¡i nhá» hÆ¡n Spark vá» ML/Batch.
- Tuning cáº§n hiá»ƒu rÃµ parallelism, slot, backpressure.

---

## 3. Case study (tiÃªu biá»ƒu)

- **Alibaba**: xá»­ lÃ½ log vÃ  giao dá»‹ch quy mÃ´ lá»›n, real-time analytics.
- **Netflix/Uber**: giÃ¡m sÃ¡t sá»± kiá»‡n há»‡ thá»‘ng, phÃ¡t hiá»‡n anomaly real-time.
- **Ververica** (sÃ¡ng láº­p Flink): nhiá»u case ngÃ¢n hÃ ng/viá»…n thÃ´ng.

---

## 4. Cáº¥u trÃºc & Kiáº¿n trÃºc há»‡ thá»‘ng demo

ThÆ° má»¥c chÃ­nh:

```
coinbase_producer.py           # Producer gá»­i giÃ¡ crypto vÃ o Kafka
docker-compose.yml             # Orchestrate toÃ n bá»™ services
Dockerfile.producer            # Build producer
Dockerfile.flink               # Build PyFlink app
Dockerfile.spark               # Build Spark streaming app (so sÃ¡nh)
init-db.sql                    # Táº¡o schema Silver/Gold
requirements.txt               # Python deps
dags/                          # Airflow DAGs (orchestrate, gold aggregations)
spark-apps/                    # Spark streaming job
flink-apps/                    # Flink streaming job
sql/                           # SQL bá»• sung/alter báº£ng
grafana/                       # Provisioning datasource & dashboards
```

Kiáº¿n trÃºc triá»ƒn khai:

```
Coinbase API â†’ Kafka â†’ (Flink & Spark) â†’ PostgreSQL (Silver) â†’ Airflow (Gold 10m/hour) â†’ Grafana

Containers: zookeeper, kafka, postgres-db, airflow-{init,webserver,scheduler},
            flink-jobmanager, flink-taskmanager, flink-crypto-processor,
            spark-master, spark-worker, spark-crypto-processor, grafana
```

---

## 5. CÃ i Ä‘áº·t, káº¿t ná»‘i vÃ  cÃ¡ch cháº¡y há»‡ thá»‘ng

YÃªu cáº§u: Docker Desktop, Windows PowerShell 5.1.

### 5.1 Khá»Ÿi Ä‘á»™ng há»‡ thá»‘ng

```powershell
# Táº¡i thÆ° má»¥c project
docker-compose up -d

# Kiá»ƒm tra containers
docker-compose ps
```

### 5.2 Kiá»ƒm tra pipeline hoáº¡t Ä‘á»™ng

```powershell
# Kiá»ƒm tra Producer
docker logs crypto-producer --tail 20

# Kiá»ƒm tra Flink JobManager UI
Start-Process "http://localhost:8082"

# Kiá»ƒm tra Spark Master UI
Start-Process "http://localhost:8081"

# Kiá»ƒm tra Silver layer (DB)
docker exec postgres-db psql -U user -d crypto_data -c "SELECT COUNT(*) FROM crypto_prices_flink;"
docker exec postgres-db psql -U user -d crypto_data -c "SELECT COUNT(*) FROM crypto_prices_realtime;"

# Kiá»ƒm tra Gold layer
docker exec postgres-db psql -U user -d crypto_data -c "SELECT COUNT(*) FROM gold_10min_metrics;"
docker exec postgres-db psql -U user -d crypto_data -c "SELECT COUNT(*) FROM gold_hourly_metrics;"
```

### 5.3 Airflow orchestration

Airflow UI: `http://localhost:8080`
- `crypto_streaming_pipeline`: (náº¿u dÃ¹ng) submit job (Spark trÆ°á»›c Ä‘Ã¢y). Hiá»‡n Spark cháº¡y báº±ng container riÃªng.
- `gold_10min_aggregation` vÃ  `gold_hourly_aggregation`: tá»•ng há»£p Gold layer theo lá»‹ch 10 phÃºt.

Táº¡o Airflow Connection cho PostgreSQL (náº¿u thiáº¿u):

```powershell
docker exec airflow-webserver airflow connections add 'postgres_crypto' --conn-type 'postgres' --conn-host 'postgres-db' --conn-schema 'crypto_data' --conn-login 'user' --conn-password 'password' --conn-port 5432
```

### 5.4 Grafana dashboard

```powershell
docker-compose up -d grafana
Start-Process "http://localhost:3000"
# ÄÄƒng nháº­p: admin / admin
```

Dashboard: `Real-time Crypto Pipeline` (UID `crypto-pipeline`) hiá»ƒn thá»‹:
- GiÃ¡ real-time (Spark/Flink), Ä‘á»™ trá»… trung bÃ¬nh (5 phÃºt), tá»•ng sá»‘ records, phÃ¢n bá»‘ theo symbol.
- Gold layer: Avg price theo giá», % thay Ä‘á»•i giÃ¡.

---

## 6. So sÃ¡nh Flink vá»›i Spark

### 6.1 Kiáº¿n trÃºc & ngá»¯ nghÄ©a
- **Flink**: event-time native, watermarks, stateful streaming, exactly-once máº¡nh.
- **Spark Structured Streaming**: micro-batch, latency cao hÆ¡n, máº¡nh vá» batch/SQL/ML.

### 6.2 Triá»ƒn khai trong há»‡ thá»‘ng
- Flink cháº¡y trong container `flink-crypto-processor` (PyFlink), Ä‘á»c Kafka vÃ  ghi PostgreSQL.
- Spark cháº¡y trong container `spark-crypto-processor`, Ä‘á»c Kafka vÃ  ghi PostgreSQL.
- Cáº£ hai cÃ¹ng Ä‘á»c `topic: crypto_prices` Ä‘á»ƒ so sÃ¡nh cÃ´ng báº±ng.

### 6.3 Thá»±c nghiá»‡m Ä‘o lÆ°á»ng (script `compare_latency.ps1`)

Cháº¡y:
```powershell
& "$PWD\compare_latency.ps1"
```
CÃ¡c truy váº¥n: tá»•ng records, latency trung bÃ¬nh/percentiles, throughput, freshness, time-series.

Káº¿t quáº£ tiÃªu biá»ƒu (máº«u):
- Spark avg latency (5m): ~9.0s; Flink avg latency (5m): ~2.1s.
- Throughput: Spark ~40 rec/min; Flink ~25â€“30 rec/min (tÃ¹y thá»i Ä‘iá»ƒm).
- Percentiles: Flink p95 ~3.5s vs Spark p95 ~14s.

---

## 7. Äiá»u chá»‰nh tham sá»‘ (tuning)

### 7.1 Flink
- Parallelism: `flink-apps/flink_stream_processor.py` (set `env.set_parallelism(2)`), `taskmanager.numberOfTaskSlots` trong `docker-compose.yml`.
- Checkpointing: báº­t trong job PyFlink (náº¿u cáº§n), cáº¥u hÃ¬nh interval vÃ  state backend.
- Kafka source: `properties` nhÆ° `group.id`, `auto.offset.reset`, `scan.startup.mode`.

### 7.2 Spark
- Worker resources: `SPARK_WORKER_CORES`, `SPARK_WORKER_MEMORY` trong `docker-compose.yml`.
- Batch interval vÃ  trigger: trong `spark-apps/spark_stream_processor.py`.
- Kafka options: maxOffsetsPerTrigger, startingOffsets.

### 7.3 PostgreSQL
- Indexes: Ä‘Ã£ táº¡o trÃªn timestamp, symbol.
- Connection pool: cÃ³ thá»ƒ tinh chá»‰nh náº¿u throughput cao.

---

## 8. Minh há»a váº­n hÃ nh xá»­ lÃ½ dá»¯ liá»‡u lá»›n

Pipeline minh há»a Ä‘áº§y Ä‘á»§ cÃ¡c bÆ°á»›c:
1) Producer â†’ Kafka (ingestion liÃªn tá»¥c má»—i 10s cho 5 cáº·p crypto).
2) Flink & Spark â†’ PostgreSQL (Silver): lÆ°u sá»± kiá»‡n kÃ¨m `processed_at` Ä‘á»ƒ Ä‘o latency.
3) Airflow â†’ Gold (10 phÃºt/giá»): tá»•ng há»£p avg/min/max/volatility, % thay Ä‘á»•i.
4) Grafana â†’ Dashboard real-time: so sÃ¡nh Spark vs Flink.

CÃ³ thá»ƒ tÄƒng táº£i báº±ng cÃ¡ch:
- TÄƒng `POLL_INTERVAL_SECONDS` nhá» láº¡i (vÃ­ dá»¥ 5s).
- TÄƒng sá»‘ symbol, partitions Kafka, parallelism Flink/Spark.

---

## 9. Káº¿t quáº£ thá»±c nghiá»‡m

Trong phiÃªn cháº¡y hiá»‡n táº¡i:
- Silver (Spark): ~1.8k records; Silver (Flink): ~2.0k records.
- Gold 10m: cÃ³ dá»¯ liá»‡u (10+ báº£n ghi); Gold hourly: cÃ³ dá»¯ liá»‡u (5+ báº£n ghi).
- Latency: Flink tháº¥p hÆ¡n Ä‘Ã¡ng ká»ƒ (p50 ~1.5s vs ~9.5s).

Chi tiáº¿t xem táº¡i script `compare_latency.ps1` vÃ  dashboard Grafana.

---

## 10. Káº¿t luáº­n

- **Flink** phÃ¹ há»£p cho streaming thá»i gian thá»±c vá»›i yÃªu cáº§u Ä‘á»™ trá»… tháº¥p, exactly-once vÃ  event-time chuáº©n.
- **Spark** máº¡nh á»Ÿ batch, SQL, ML, vÃ  há»‡ sinh thÃ¡i phong phÃº; streaming micro-batch cÃ³ Ä‘á»™ trá»… cao hÆ¡n.
- Há»‡ thá»‘ng demo minh há»a rÃµ rÃ ng kiáº¿n trÃºc káº¿t há»£p, cÃ¡ch cÃ i Ä‘áº·t/cháº¡y, Ä‘iá»u chá»‰nh tham sá»‘, vÃ  so sÃ¡nh thá»±c nghiá»‡m.

---

## Phá»¥ lá»¥c: Lá»‡nh nhanh

```powershell
# Khá»Ÿi Ä‘á»™ng toÃ n bá»™
docker-compose up -d

# Kiá»ƒm tra DB nhanh
docker exec postgres-db psql -U user -d crypto_data -c "SELECT COUNT(*) FROM crypto_prices_flink;"

# Cháº¡y script so sÃ¡nh
& "$PWD\compare_latency.ps1"

# Má»Ÿ UIs
Start-Process "http://localhost:8082"   # Flink
Start-Process "http://localhost:8081"   # Spark
Start-Process "http://localhost:8080"   # Airflow
Start-Process "http://localhost:3000"   # Grafana
```

## ğŸ—ï¸ Kiáº¿n trÃºc há»‡ thá»‘ng

### Data Flow Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   CRYPTOCURRENCY DATA PIPELINE                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[1] DATA INGESTION
    Coinbase API (REST)
         â”‚
         â”œâ”€ GET /products/{symbol}/ticker
         â”‚  â””â”€ Response: {"price": "86746.075", "time": "2024-11-24T..."}
         â”‚
         â–¼
    Producer (Python + kafka-python)
         â”‚
         â”œâ”€ Poll interval: 10 seconds
         â”œâ”€ Symbols: BTC, ETH, SOL, ADA, DOGE
         â”‚
         â–¼
    [JSON Message]

[2] MESSAGE BROKER
         â”‚
         â–¼
    Apache Kafka (Topic: crypto_prices)
         â”‚
         â”œâ”€ Partitions: 3 (for parallelism)
         â”‚
         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                â”‚                â”‚
         â–¼                â–¼                â–¼
    Partition 0     Partition 1      Partition 2

[3] DUAL STREAM PROCESSING (PARALLEL)

    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚    SPARK STREAMING          â”‚  â”‚    FLINK STREAMING          â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ â€¢ Micro-batch (15s trigger) â”‚  â”‚ â€¢ Event-driven processing   â”‚
    â”‚ â€¢ DataFrame API             â”‚  â”‚ â€¢ Table API + SQL DDL       â”‚
    â”‚ â€¢ foreachBatch â†’ JDBC       â”‚  â”‚ â€¢ JDBC Connector Sink       â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                                    â”‚
         â–¼                                    â–¼

[4] DATA STORAGE
    PostgreSQL Database (crypto_data)
         â”‚
         â”œâ”€ crypto_prices_realtime (Spark writes)
         â”œâ”€ crypto_prices_flink (Flink writes)
         â”œâ”€ gold_hourly_metrics (aggregated)
         â””â”€ gold_10min_metrics (aggregated)
```

### Infrastructure Components

| Component | Technology | Version | Port | Purpose |
|-----------|------------|---------|------|---------|
| **Message Broker** | Apache Kafka | 7.3.0 | 9092 | Stream data distribution |
| **Coordination** | Zookeeper | 7.3.0 | 2181 | Kafka coordination |
| **Stream Engine 1** | Apache Spark | 3.5.0 | 8081 | Micro-batch processing |
| **Stream Engine 2** | Apache Flink | 1.18.0 | 8082 | True streaming |
| **Database** | PostgreSQL | 14 | 5432 | Data persistence |
| **Orchestration** | Apache Airflow | 2.8.1 | 8080 | Workflow management |
| **Producer** | Python | 3.11 | - | Data ingestion |

---

## ğŸ› ï¸ CÃ´ng nghá»‡ sá»­ dá»¥ng

### Core Technologies

```yaml
Stream Processing:
  - Apache Spark Structured Streaming 3.5.0
  - Apache Flink DataStream/Table API 1.18.0
  
Message Broker:
  - Apache Kafka 7.3.0
  - Zookeeper 7.3.0
  
Database:
  - PostgreSQL 14
  
Orchestration:
  - Apache Airflow 2.8.1
  
Programming:
  - Python 3.11
  - PyFlink 1.18.0
  - kafka-python 2.0.2
  
Infrastructure:
  - Docker & Docker Compose
  - Linux Containers
```

### Why These Technologies?

**Apache Spark**: Industry standard cho batch + streaming, mature ecosystem  
**Apache Flink**: True streaming vá»›i ultra-low latency, exactly-once semantics  
**Kafka**: High-throughput, fault-tolerant message broker  
**PostgreSQL**: ACID-compliant, perfect for analytics  
**Airflow**: Python-native orchestration, easy DAG management  

---

## ğŸš€ Quick Start - Khá»Ÿi Ä‘á»™ng nhanh

### Prerequisites

- Docker Desktop (Windows/Mac/Linux)
- 8GB RAM minimum (16GB recommended)
- 20GB disk space
- Internet connection

### BÆ°á»›c 1: Clone Repository

```powershell
git clone https://github.com/vngbthang/real-time-cryptocurrency-data-pipeline-flink-is405.git
cd real-time-cryptocurrency-data-pipeline-flink-is405
```

### BÆ°á»›c 2: Start toÃ n bá»™ há»‡ thá»‘ng

```powershell
docker-compose up -d
```

**Chá» 2-3 phÃºt** Ä‘á»ƒ táº¥t cáº£ services khá»Ÿi Ä‘á»™ng.

### BÆ°á»›c 3: Verify há»‡ thá»‘ng

```powershell
# Check all containers running
docker-compose ps
```

Káº¿t quáº£ mong Ä‘á»£i: **14 containers** vá»›i status `Up`:
- âœ… zookeeper
- âœ… kafka
- âœ… postgres-db
- âœ… postgres-airflow-db
- âœ… crypto-producer
- âœ… spark-master
- âœ… spark-worker
- âœ… flink-jobmanager
- âœ… flink-taskmanager
- âœ… flink-crypto-processor
- âœ… airflow-init
- âœ… airflow-webserver
- âœ… airflow-scheduler

### BÆ°á»›c 4: Kiá»ƒm tra Producer

```powershell
docker logs crypto-producer --tail 20
```

Káº¿t quáº£ mong Ä‘á»£i:
```
âœ… BTC-USD      Price: $   86,865.54
âœ… ETH-USD      Price: $    2,833.05
âœ… SOL-USD      Price: $      130.35
âœ… ADA-USD      Price: $        0.41
âœ… DOGE-USD     Price: $        0.15
ğŸ“Š Summary: 5/5 pairs sent successfully
```

### BÆ°á»›c 5: Verify dá»¯ liá»‡u trong Database

```powershell
docker exec postgres-db psql -U user -d crypto_data -c "SELECT 'Spark' as engine, COUNT(*) FROM crypto_prices_realtime UNION ALL SELECT 'Flink' as engine, COUNT(*) FROM crypto_prices_flink;"
```

Káº¿t quáº£ sau vÃ i phÃºt:
```
 engine | count
--------+-------
 Spark  |   75+
 Flink  |   50+
```

### BÆ°á»›c 6: Cháº¡y Performance Test

```powershell
.\compare_latency.ps1
```

**Káº¿t quáº£ mong Ä‘á»£i:** Flink nhanh hÆ¡n Spark **3-5 láº§n**.

### BÆ°á»›c 7: Truy cáº­p Dashboards

- **Airflow UI:** http://localhost:8080 (admin/admin)
- **Spark Master UI:** http://localhost:8081
- **Flink JobManager UI:** http://localhost:8082

---

## ğŸ“Š Apache Flink - Giá»›i thiá»‡u chi tiáº¿t

### Kiáº¿n trÃºc Apache Flink

Apache Flink lÃ  má»™t **distributed stream processing framework** Ä‘Æ°á»£c thiáº¿t káº¿ cho xá»­ lÃ½ real-time data vá»›i latency cá»±c tháº¥p.

#### Kiáº¿n trÃºc cá»‘t lÃµi

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    APACHE FLINK ARCHITECTURE                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[1] CLIENT LAYER
    Flink Application (Python/Java/Scala)
         â”‚
         â”œâ”€ DataStream API (imperative)
         â”œâ”€ Table API (declarative)
         â””â”€ SQL API (declarative)
         â”‚
         â–¼ Submit Job
         
[2] CONTROL PLANE
    JobManager (Master)
         â”‚
         â”œâ”€ JobGraph â†’ ExecutionGraph
         â”œâ”€ Resource Management
         â”œâ”€ Checkpoint Coordination
         â””â”€ Task Scheduling
         â”‚
         â–¼ Distribute Tasks
         
[3] DATA PLANE
    TaskManager 1       TaskManager 2       TaskManager 3
    â”œâ”€ Task Slot 1      â”œâ”€ Task Slot 1      â”œâ”€ Task Slot 1
    â”œâ”€ Task Slot 2      â”œâ”€ Task Slot 2      â”œâ”€ Task Slot 2
    â””â”€ Task Slot 3      â””â”€ Task Slot 3      â””â”€ Task Slot 3
         â”‚                   â”‚                   â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
[4] STATE MANAGEMENT
    State Backend (RocksDB / Heap)
         â”‚
         â”œâ”€ Keyed State (per key)
         â”œâ”€ Operator State (per parallel instance)
         â””â”€ Checkpoints (distributed snapshots)
```

#### Core Components

| Component | Vai trÃ² | Sá»‘ lÆ°á»£ng | Docker Service |
|-----------|---------|----------|----------------|
| **JobManager** | Master node, orchestration | 1 | flink-jobmanager |
| **TaskManager** | Worker node, execute tasks | 1+ | flink-taskmanager |
| **Task Slot** | Thread unit for parallelism | N Ã— TaskManager | Configured in env |
| **State Backend** | Persistent storage cho state | 1 (shared) | RocksDB/Heap |

### Æ¯u Ä‘iá»ƒm vÃ  NhÆ°á»£c Ä‘iá»ƒm

#### Æ¯u Ä‘iá»ƒm

| Æ¯u Ä‘iá»ƒm | MÃ´ táº£ | Use Case |
|---------|-------|----------|
| **Low Latency** | Xá»­ lÃ½ sub-second latency | Real-time fraud detection, HFT trading |
| **High Throughput** | Millions events/second | IoT data ingestion, log processing |
| **Exactly-Once** | Strong consistency guarantees | Financial transactions, billing systems |
| **Stateful Processing** | Built-in state management | Session analytics, pattern detection |
| **Event Time Processing** | Handle out-of-order events | Time-series analytics, late data handling |
| **Flexible Deployment** | Standalone, YARN, K8s, Mesos | Cloud-native or on-premise |
| **SQL Support** | Table API & SQL for streaming | Business analysts, rapid development |
| **Savepoints** | Version control for streaming apps | A/B testing, rolling updates |

#### NhÆ°á»£c Ä‘iá»ƒm

| NhÆ°á»£c Ä‘iá»ƒm | MÃ´ táº£ | Mitigation |
|-----------|-------|------------|
| **Steep Learning Curve** | Concepts phá»©c táº¡p (watermarks, state, checkpoints) | Báº¯t Ä‘áº§u vá»›i Table API trÆ°á»›c DataStream API |
| **Memory Intensive** | State backend cáº§n nhiá»u RAM | DÃ¹ng RocksDB cho large state, tune memory configs |
| **Operational Complexity** | Cáº§n monitoring checkpoint lag, backpressure | DÃ¹ng Flink Dashboard + Prometheus metrics |
| **Limited ML Support** | KhÃ´ng cÃ³ ML library nhÆ° Spark MLlib | TÃ­ch há»£p vá»›i TensorFlow, PyTorch riÃªng |
| **Smaller Ecosystem** | Ãt connectors hÆ¡n Spark | Community Ä‘ang phÃ¡t triá»ƒn nhanh |
| **Debugging Challenges** | Distributed debugging khÃ³ | DÃ¹ng local mode + extensive logging |

---

## âš–ï¸ So sÃ¡nh Apache Spark vs Apache Flink

### Kiáº¿n trÃºc xá»­ lÃ½

| TiÃªu chÃ­ | Apache Spark Structured Streaming | Apache Flink |
|----------|-----------------------------------|--------------|
| **Processing Model** | Micro-batch (15 giÃ¢y/batch) | True streaming (event-by-event) |
| **Core Abstraction** | RDD â†’ DataFrame/Dataset | DataStream â†’ Table |
| **State Management** | External state stores (HDFS, S3) | Built-in managed state (RocksDB) |
| **Latency** | Seconds (batch interval) | Milliseconds (event-driven) |
| **Throughput** | Excellent for large batches | Excellent for continuous streams |
| **Memory Model** | In-memory caching for speed | Streaming pipelined execution |
| **Fault Tolerance** | RDD lineage + checkpointing | Distributed snapshots (Chandy-Lamport) |

### API Comparison

**Spark Structured Streaming:**
```python
# Declarative API vá»›i DataFrame
from pyspark.sql import SparkSession
from pyspark.sql.functions import from_json, col

spark = SparkSession.builder.appName("CryptoStream").getOrCreate()

# Read stream
df = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "kafka:9092") \
    .option("subscribe", "crypto_prices") \
    .load()

# Transform (batch-like operations)
crypto_df = df.selectExpr("CAST(value AS STRING)") \
    .select(from_json(col("value"), schema).alias("data")) \
    .select("data.*")

# Write stream vá»›i trigger interval
query = crypto_df.writeStream \
    .outputMode("complete") \
    .trigger(processingTime="15 seconds") \
    .format("console") \
    .start()
```

**Flink Table API + SQL:**
```python
# DDL-style table creation
from pyflink.table import StreamTableEnvironment

table_env = StreamTableEnvironment.create(env)

# Kafka source
table_env.execute_sql("""
    CREATE TABLE crypto_source (
        symbol STRING,
        price DOUBLE,
        `timestamp` BIGINT,
        WATERMARK FOR event_time AS TO_TIMESTAMP_LTZ(`timestamp`, 3)
    ) WITH (
        'connector' = 'kafka',
        'topic' = 'crypto_prices',
        'properties.bootstrap.servers' = 'kafka:9092',
        'format' = 'json',
        'scan.startup.mode' = 'latest-offset'
    )
""")

# JDBC sink
table_env.execute_sql("""
    CREATE TABLE crypto_sink (
        symbol STRING,
        price DOUBLE,
        `user` STRING,
        `timestamp` BIGINT
    ) WITH (
        'connector' = 'jdbc',
        'url' = 'jdbc:postgresql://postgres-db:5432/crypto_data',
        'table-name' = 'crypto_prices_flink',
        'username' = 'user',
        'password' = 'password'
    )
""")

# Streaming query
table_env.execute_sql("INSERT INTO crypto_sink SELECT * FROM crypto_source")
```

### Performance Comparison

| Metric | Spark (Micro-batch 15s) | Flink (True Streaming) |
|--------|-------------------------|------------------------|
| **Latency** | 8-9 giÃ¢y | 1-3 giÃ¢y |
| **Throughput** | ~27 records/minute | ~26 records/minute |
| **Memory Usage** | 2-4 GB (executor heap) | 1-3 GB (task manager) |
| **CPU Usage** | Spiky (batch processing) | Smooth (continuous) |
| **Exactly-Once** | âœ… Vá»›i foreachBatch | âœ… Native support |
| **Late Data Handling** | âš ï¸ Limited watermark support | âœ… Advanced watermark strategies |

### Time Semantics

**Spark:**
```python
# Processing time (khi data Ä‘áº¿n Spark)
df.writeStream \
    .trigger(processingTime="15 seconds") \
    .start()

# Event time (limited support)
df.withWatermark("timestamp", "10 minutes")
```

**Flink:**
```python
# Event time vá»›i watermark strategy
from pyflink.common.watermark_strategy import WatermarkStrategy
from pyflink.common.time import Duration

strategy = WatermarkStrategy \
    .for_bounded_out_of_orderness(Duration.of_seconds(5)) \
    .with_timestamp_assigner(lambda event, ts: event['timestamp'])

stream.assign_timestamps_and_watermarks(strategy)
```

### State Management

| Feature | Spark | Flink |
|---------|-------|-------|
| **State Store** | External (HDFS/S3) | Embedded (RocksDB/Memory) |
| **State Size** | Limited by batch size | Unlimited (RocksDB disk) |
| **State Access** | Batch-based | Continuous access |
| **Checkpointing** | Incremental (Delta files) | Asynchronous barriers |
| **Recovery Time** | Minutes (batch replay) | Seconds (state restore) |

### Windowing Capabilities

**Spark (Limited):**
```python
# Fixed windows only
df.groupBy(
    window("timestamp", "5 minutes")
).count()
```

**Flink (Comprehensive):**
```python
# Tumbling Window
stream.key_by(...).window(TumblingEventTimeWindows.of(Time.minutes(5)))

# Sliding Window
stream.key_by(...).window(SlidingEventTimeWindows.of(
    Time.minutes(10),  # size
    Time.minutes(5)    # slide
))

# Session Window (activity gap-based)
stream.key_by(...).window(EventTimeSessionWindows.with_gap(Time.minutes(30)))

# Global Window vá»›i custom triggers
stream.key_by(...).window(GlobalWindows.create()).trigger(...)
```

---

## ğŸ”§ Äiá»u chá»‰nh tham sá»‘ Flink

### Cáº¥u hÃ¬nh trong docker-compose.yml

```yaml
flink-jobmanager:
  image: flink:1.18.0-scala_2.12-java11
  environment:
    - |
      FLINK_PROPERTIES=
      # === PARALLELISM & SLOTS ===
      taskmanager.numberOfTaskSlots: 4           # Sá»‘ task slots má»—i TaskManager
      parallelism.default: 2                     # Parallelism máº·c Ä‘á»‹nh
      
      # === MEMORY CONFIGURATION ===
      taskmanager.memory.process.size: 2048m     # Tá»•ng memory cho TaskManager
      taskmanager.memory.flink.size: 1536m       # Flink managed memory
      
      # === CHECKPOINT SETTINGS ===
      execution.checkpointing.interval: 60000    # Checkpoint má»—i 60 giÃ¢y
      execution.checkpointing.mode: EXACTLY_ONCE # At-least-once hoáº·c exactly-once
      
      # === STATE BACKEND ===
      state.backend: rocksdb                     # rocksdb hoáº·c filesystem
      state.checkpoints.dir: file:///tmp/flink-checkpoints
```

### Performance Tuning Parameters

**1. Parallelism (Äá»™ song song)**
```python
env = StreamExecutionEnvironment.get_execution_environment()

# Set global parallelism
env.set_parallelism(4)

# Set per-operator parallelism
stream.map(my_function).set_parallelism(8)
```

**NguyÃªn táº¯c:** `parallelism = sá»‘ TaskManager Ã— sá»‘ slots per TaskManager`  
**Demo nÃ y:** 3 Kafka partitions â†’ parallelism=2 hoáº·c 3

**2. Checkpointing (Fault Tolerance)**
```python
# Enable checkpointing
env.enable_checkpointing(60000)  # 60 seconds

# Checkpoint configuration
checkpoint_config = env.get_checkpoint_config()
checkpoint_config.set_checkpointing_mode(CheckpointingMode.EXACTLY_ONCE)
checkpoint_config.set_checkpoint_timeout(600000)  # 10 min timeout
```

**3. State Backend Selection**

| State Backend | Use Case | Max Size | Performance |
|---------------|----------|----------|-------------|
| **HashMap** | Small state (<100MB) | Limited by heap | Very fast |
| **RocksDB** | Large state (GBs-TBs) | Disk-bounded | Moderate (disk I/O) |

**4. Buffer Timeout (Latency Tuning)**
```python
env.set_buffer_timeout(100)  # milliseconds
```

| Buffer Timeout | Latency | Throughput | Use Case |
|----------------|---------|------------|----------|
| 0ms | Lowest | Lowest | Ultra-low latency apps |
| 100ms | Low | High | Balanced (recommended) |
| -1 (disabled) | Highest | Highest | Batch-like processing |

**5. Kafka Consumer Configuration**
```python
table_env.execute_sql("""
    CREATE TABLE crypto_source (...) WITH (
        'connector' = 'kafka',
        'properties.group.id' = 'flink-crypto-consumer',
        'scan.startup.mode' = 'latest-offset',
        'properties.fetch.min.bytes' = '1024',
        'properties.max.partition.fetch.bytes' = '1048576'
    )
""")
```

**6. JDBC Sink Tuning**
```python
table_env.execute_sql("""
    CREATE TABLE crypto_sink (...) WITH (
        'connector' = 'jdbc',
        'sink.buffer-flush.max-rows' = '100',         # Batch size
        'sink.buffer-flush.interval' = '1s',          # Flush interval
        'sink.max-retries' = '3',                     # Retry on failure
        'sink.parallelism' = '2'                      # Writer parallelism
    )
""")
```

---

## ğŸ“ˆ Performance Verification - Báº±ng chá»©ng Flink nhanh hÆ¡n

### Cháº¡y Performance Test

```powershell
.\compare_latency.ps1
```

Script nÃ y cháº¡y 4 tests Ä‘á»ƒ Ä‘o vÃ  so sÃ¡nh hiá»‡u suáº¥t giá»¯a Spark vÃ  Flink.

### Test 1: Average Latency

**Äo latency trung bÃ¬nh trong 5 phÃºt gáº§n Ä‘Ã¢y:**

```sql
SELECT 
    engine,
    AVG(processed_at_timestamp - producer_timestamp) as avg_latency_sec,
    COUNT(*) as sample_size
FROM (Spark table UNION Flink table)
WHERE processed_at > NOW() - INTERVAL '5 minutes';
```

**Káº¿t quáº£:**
```
 engine | avg_latency_sec | sample_size 
--------+-----------------+-------------
 Spark  |            8.83 |         120
 Flink  |            2.23 |         125
```

**PhÃ¢n tÃ­ch:**
- âœ… **Flink nhanh hÆ¡n 3.96x** (8.83s vs 2.23s)
- Spark: 8-9 giÃ¢y latency do micro-batch processing
- Flink: 2-3 giÃ¢y latency nhá» event-driven architecture

### Test 2: Latest Records Latency

**5 records má»›i nháº¥t tá»« má»—i engine:**

**Spark:**
```
  symbol  | latency_sec | db_time  
----------+-------------+----------
 DOGE-USD |           7 | 09:09:00
 ADA-USD  |           7 | 09:09:00
 SOL-USD  |           7 | 09:09:00
 ETH-USD  |           7 | 09:09:00
 BTC-USD  |           7 | 09:09:00
```

**Flink:**
```
  symbol  | latency_sec | db_time  
----------+-------------+----------
 DOGE-USD |           4 | 09:09:08
 ADA-USD  |           3 | 09:09:07
 SOL-USD  |           2 | 09:09:06
 ETH-USD  |           1 | 09:09:05
 BTC-USD  |           1 | 09:09:05
```

**PhÃ¢n tÃ­ch:**
- Spark: Táº¥t cáº£ records **cÃ¹ng latency (7s)** vÃ¬ batch processing
- Flink: Latency **khÃ¡c nhau (1-4s)** vÃ¬ xá»­ lÃ½ tá»«ng event
- âœ… **Flink nhanh hÆ¡n 5-7x**

### Test 3: Throughput Comparison

```
 engine |     records_per_min     
--------+-------------------------
 Spark  | 26.67
 Flink  | 26.11
```

**PhÃ¢n tÃ­ch:** âœ… **Throughput tÆ°Æ¡ng Ä‘Æ°Æ¡ng** (~26 records/min)

### Test 4: Data Freshness

```
 engine | time_since_last_write 
--------+-----------------------
 Spark  | 00:00:15.77
 Flink  | 00:00:06.91
```

**PhÃ¢n tÃ­ch:**
- Spark: Data cÅ© hÆ¡n **15.77 giÃ¢y**
- Flink: Data chá»‰ cÅ© **6.91 giÃ¢y**
- âœ… **Flink data má»›i hÆ¡n 2.3x**

### Giáº£i thÃ­ch táº¡i sao Flink nhanh hÆ¡n

#### Spark Micro-batch Processing

```
Timeline:
00:00  Producer sends â†’ Kafka
00:00  â”œâ”€ Message arrives in Kafka
00:00  â”œâ”€ Spark: Waiting for trigger (15s interval)
00:15  â””â”€ Trigger! Read all messages from last 15s
00:16      â”œâ”€ Parse JSON
00:17      â”œâ”€ Transform data
00:18      â””â”€ Write batch to PostgreSQL
       
Total Latency: 15-18 seconds
```

**NguyÃªn nhÃ¢n cháº­m:**
- â±ï¸ **Trigger Interval = 15 giÃ¢y:** Pháº£i Ä‘á»£i Ä‘á»§ thá»i gian má»›i xá»­ lÃ½
- ğŸ“¦ **Batch Processing:** Táº¥t cáº£ messages trong 15s Ä‘Æ°á»£c xá»­ lÃ½ cÃ¹ng lÃºc
- **Minimum Latency = Trigger Interval**

#### Flink Event-Driven Processing

```
Timeline:
00:00  Producer sends â†’ Kafka
00:00  â”œâ”€ Message arrives in Kafka
00:01  â”œâ”€ Flink reads event immediately
00:01  â”œâ”€ Parse JSON (in-flight)
00:02  â”œâ”€ Transform data (in-flight)
00:02  â””â”€ Write to PostgreSQL immediately

Total Latency: 1-3 seconds
```

**NguyÃªn nhÃ¢n nhanh:**
- âš¡ **Event-Driven:** Xá»­ lÃ½ ngay khi message Ä‘áº¿n
- ğŸ”„ **Pipelined Execution:** Parse â†’ Transform â†’ Write song song
- ğŸ’¨ **No Waiting:** KhÃ´ng cÃ³ trigger interval

### Báº£ng tÃ³m táº¯t Performance

| Metric | Spark | Flink | Winner |
|--------|-------|-------|--------|
| **Avg Latency** | 8.83s | 2.23s | âœ… Flink (3.96x) |
| **Min Latency** | 15s | 1s | âœ… Flink (15x) |
| **Throughput** | 26.67 rec/min | 26.11 rec/min | âš–ï¸ Equal |
| **Data Freshness** | 15.77s old | 6.91s old | âœ… Flink (2.3x) |

---

## ğŸ¯ Káº¿t luáº­n & Lá»±a chá»n

### Khi nÃ o dÃ¹ng Flink?

âœ… **Real-time dashboards:** Cáº§n update < 5 giÃ¢y  
âœ… **Fraud detection:** PhÃ¡t hiá»‡n gian láº­n ngay láº­p tá»©c  
âœ… **Live monitoring:** GiÃ¡m sÃ¡t há»‡ thá»‘ng real-time  
âœ… **Trading systems:** High-frequency trading  
âœ… **IoT streaming:** Sensor data processing  
âœ… **Alerting systems:** Gá»­i alert trong vÃ i giÃ¢y  

### Khi nÃ o dÃ¹ng Spark?

âœ… **ETL pipelines:** Batch + streaming trong cÃ¹ng code  
âœ… **Data warehousing:** Load data má»—i 15-30 phÃºt  
âœ… **Machine Learning:** Training models trÃªn streaming data  
âœ… **Report generation:** Táº¡o bÃ¡o cÃ¡o Ä‘á»‹nh ká»³  
âœ… **Large batch jobs:** Xá»­ lÃ½ terabytes data  

### Báº£ng lá»±a chá»n

| TiÃªu chÃ­ | Spark | Flink | Chá»n gÃ¬? |
|----------|-------|-------|----------|
| **Latency requirement** | 10-30s OK | < 5s cáº§n | Flink cho real-time |
| **Data volume** | Terabytes | Gigabytes | Spark cho big batch |
| **Team experience** | Spark ecosystem | Flink learning curve | Spark dá»… hÆ¡n |
| **Use case** | Analytics, ML | Monitoring, alerting | Depends |
| **Cost** | Lower (batch efficient) | Higher (always running) | Spark ráº» hÆ¡n |

---

## ğŸ›‘ Stop System

```powershell
# Stop all containers
docker-compose down

# Stop and remove volumes (clean slate)
docker-compose down -v
```

---

## ğŸ“ Káº¿t luáº­n tá»•ng quan

**Apache Spark Structured Streaming** vÃ  **Apache Flink** Ä‘á»u lÃ  cÃ´ng cá»¥ máº¡nh máº½ cho xá»­ lÃ½ streaming:

- **Spark**: PhÃ¹ há»£p cho batch + streaming, latency 8-15 giÃ¢y, dá»… há»c náº¿u Ä‘Ã£ biáº¿t Spark ecosystem
- **Flink**: Latency tháº¥p 1-3 giÃ¢y, event-driven, phá»©c táº¡p hÆ¡n nhÆ°ng máº¡nh máº½ cho real-time analytics

**Káº¿t quáº£ thá»±c táº¿ tá»« demo nÃ y:**
- Producer gá»­i 5 crypto pairs má»—i 10 giÃ¢y
- Spark xá»­ lÃ½ theo batch 15 giÃ¢y â†’ **latency 8.83s**
- Flink xá»­ lÃ½ real-time tá»«ng event â†’ **latency 2.23s**
- Cáº£ hai Ä‘á»u ghi vÃ o PostgreSQL Ä‘á»ƒ so sÃ¡nh side-by-side

**Báº±ng chá»©ng cá»¥ thá»ƒ:** Cháº¡y `.\compare_latency.ps1` Ä‘á»ƒ xem Flink nhanh hÆ¡n Spark **3.96 láº§n**.

**Lá»±a chá»n phá»¥ thuá»™c vÃ o:** YÃªu cáº§u latency, data volume, team experience, vÃ  budget.

