# ğŸ’° Real-Time Cryptocurrency Pipeline: Apache Spark vs Apache Flink

> **Dá»± Ã¡n IS405**: So sÃ¡nh hiá»‡u suáº¥t xá»­ lÃ½ dá»¯ liá»‡u streaming giá»¯a **Apache Spark Structured Streaming** vÃ  **Apache Flink** trÃªn pipeline thu tháº­p giÃ¡ cryptocurrency real-time tá»« Coinbase API.

[![Docker](https://img.shields.io/badge/Docker-Ready-blue)](https://www.docker.com/)
[![Spark](https://img.shields.io/badge/Spark-3.5.0-orange)](https://spark.apache.org/)
[![Flink](https://img.shields.io/badge/Flink-1.18.0-red)](https://flink.apache.org/)
[![Kafka](https://img.shields.io/badge/Kafka-7.3.0-black)](https://kafka.apache.org/)

---

## ğŸ“‹ Má»¥c lá»¥c

1. [Giá»›i thiá»‡u](#-giá»›i-thiá»‡u)
2. [Kiáº¿n trÃºc há»‡ thá»‘ng](#-kiáº¿n-trÃºc-há»‡-thá»‘ng)
3. [CÃ´ng nghá»‡ sá»­ dá»¥ng](#-cÃ´ng-nghá»‡-sá»­-dá»¥ng)
4. [Quick Start](#-quick-start---khá»Ÿi-Ä‘á»™ng-nhanh)
5. [Apache Flink - Chi tiáº¿t](#-apache-flink---thÃ´ng-tin-chi-tiáº¿t)
6. [So sÃ¡nh Spark vs Flink](#-so-sÃ¡nh-chi-tiáº¿t-spark-vs-flink)
7. [Performance Verification](#-performance-verification---chá»©ng-minh-flink-nhanh-hÆ¡n)
8. [Dashboard & Monitoring](#-dashboard--monitoring)
9. [Troubleshooting](#-troubleshooting)
10. [Káº¿t luáº­n](#-káº¿t-luáº­n)

---

## ğŸ¯ Giá»›i thiá»‡u

Dá»± Ã¡n xÃ¢y dá»±ng má»™t **Real-Time ETL Pipeline** hoÃ n chá»‰nh Ä‘á»ƒ xá»­ lÃ½ dá»¯ liá»‡u cryptocurrency tá»« Coinbase API, vá»›i má»¥c tiÃªu chÃ­nh lÃ  **so sÃ¡nh hiá»‡u suáº¥t** giá»¯a hai stream processing engines hÃ ng Ä‘áº§u: **Apache Spark** vÃ  **Apache Flink**.

### Váº¥n Ä‘á» giáº£i quyáº¿t

- **Real-time ingestion**: Thu tháº­p dá»¯ liá»‡u giÃ¡ vÃ  khá»‘i lÆ°á»£ng giao dá»‹ch tá»« Coinbase API má»—i 10 giÃ¢y
- **Parallel stream processing**: Xá»­ lÃ½ cÃ¹ng lÃºc báº±ng cáº£ Spark vÃ  Flink Ä‘á»ƒ so sÃ¡nh
- **Latency comparison**: Äo vÃ  chá»©ng minh Flink cÃ³ latency tháº¥p hÆ¡n Spark
- **Data aggregation**: Táº¡o metrics theo cá»­a sá»• thá»i gian (10 phÃºt, 1 giá»)
- **Orchestration**: Tá»± Ä‘á»™ng hÃ³a vá»›i Apache Airflow

### 5 cáº·p cryptocurrency Ä‘Æ°á»£c theo dÃµi

```python
CRYPTO_PAIRS = [
    'BTC-USD',   # Bitcoin
    'ETH-USD',   # Ethereum
    'SOL-USD',   # Solana
    'ADA-USD',   # Cardano
    'DOGE-USD'   # Dogecoin
]
```

---

## ğŸ—ï¸ Kiáº¿n trÃºc há»‡ thá»‘ng

### Data Flow Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   CRYPTOCURRENCY DATA PIPELINE                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[1] DATA INGESTION
    Coinbase API (REST)
         â”‚
         â”œâ”€ GET /products/{symbol}/ticker
         â”‚  â””â”€ Response: {"price": "86746.075", "time": "2024-11-24T..."}
         â”‚
         â–¼
    Producer (Python + kafka-python)
         â”‚
         â”œâ”€ Poll interval: 10 seconds
         â”œâ”€ Symbols: BTC, ETH, SOL, ADA, DOGE
         â”‚
         â–¼
    [JSON Message]

[2] MESSAGE BROKER
         â”‚
         â–¼
    Apache Kafka (Topic: crypto_prices)
         â”‚
         â”œâ”€ Partitions: 3 (for parallelism)
         â”‚
         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                â”‚                â”‚
         â–¼                â–¼                â–¼
    Partition 0     Partition 1      Partition 2

[3] DUAL STREAM PROCESSING (PARALLEL)

    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚    SPARK STREAMING          â”‚  â”‚    FLINK STREAMING          â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ â€¢ Micro-batch (15s trigger) â”‚  â”‚ â€¢ Event-driven processing   â”‚
    â”‚ â€¢ DataFrame API             â”‚  â”‚ â€¢ Table API + SQL DDL       â”‚
    â”‚ â€¢ foreachBatch â†’ JDBC       â”‚  â”‚ â€¢ JDBC Connector Sink       â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                                    â”‚
         â–¼                                    â–¼

[4] DATA STORAGE
    PostgreSQL Database (crypto_data)
         â”‚
         â”œâ”€ crypto_prices_realtime (Spark writes)
         â”œâ”€ crypto_prices_flink (Flink writes)
         â”œâ”€ gold_hourly_metrics (aggregated)
         â””â”€ gold_10min_metrics (aggregated)
```

### Infrastructure Components

| Component | Technology | Version | Port | Purpose |
|-----------|------------|---------|------|---------|
| **Message Broker** | Apache Kafka | 7.3.0 | 9092 | Stream data distribution |
| **Coordination** | Zookeeper | 7.3.0 | 2181 | Kafka coordination |
| **Stream Engine 1** | Apache Spark | 3.5.0 | 8081 | Micro-batch processing |
| **Stream Engine 2** | Apache Flink | 1.18.0 | 8082 | True streaming |
| **Database** | PostgreSQL | 14 | 5432 | Data persistence |
| **Orchestration** | Apache Airflow | 2.8.1 | 8080 | Workflow management |
| **Producer** | Python | 3.11 | - | Data ingestion |

---

## ğŸ› ï¸ CÃ´ng nghá»‡ sá»­ dá»¥ng

### Core Technologies

```yaml
Stream Processing:
  - Apache Spark Structured Streaming 3.5.0
  - Apache Flink DataStream/Table API 1.18.0
  
Message Broker:
  - Apache Kafka 7.3.0
  - Zookeeper 7.3.0
  
Database:
  - PostgreSQL 14
  
Orchestration:
  - Apache Airflow 2.8.1
  
Programming:
  - Python 3.11
  - PyFlink 1.18.0
  - kafka-python 2.0.2
  
Infrastructure:
  - Docker & Docker Compose
  - Linux Containers
```

### Why These Technologies?

**Apache Spark**: Industry standard cho batch + streaming, mature ecosystem  
**Apache Flink**: True streaming vá»›i ultra-low latency, exactly-once semantics  
**Kafka**: High-throughput, fault-tolerant message broker  
**PostgreSQL**: ACID-compliant, perfect for analytics  
**Airflow**: Python-native orchestration, easy DAG management  

---

## ğŸš€ Quick Start - Khá»Ÿi Ä‘á»™ng nhanh

### Prerequisites

- Docker Desktop (Windows/Mac/Linux)
- 8GB RAM minimum (16GB recommended)
- 20GB disk space
- Internet connection

### BÆ°á»›c 1: Clone Repository

```powershell
git clone https://github.com/vngbthang/real-time-cryptocurrency-data-pipeline-flink-is405.git
cd real-time-cryptocurrency-data-pipeline-flink-is405
```

### BÆ°á»›c 2: Start toÃ n bá»™ há»‡ thá»‘ng

```powershell
docker-compose up -d
```

**Chá» 2-3 phÃºt** Ä‘á»ƒ táº¥t cáº£ services khá»Ÿi Ä‘á»™ng.

### BÆ°á»›c 3: Verify há»‡ thá»‘ng

```powershell
# Check all containers running
docker-compose ps
```

Káº¿t quáº£ mong Ä‘á»£i: **14 containers** vá»›i status `Up`:
- âœ… zookeeper
- âœ… kafka
- âœ… postgres-db
- âœ… postgres-airflow-db
- âœ… crypto-producer
- âœ… spark-master
- âœ… spark-worker
- âœ… flink-jobmanager
- âœ… flink-taskmanager
- âœ… flink-crypto-processor
- âœ… airflow-init
- âœ… airflow-webserver
- âœ… airflow-scheduler

### BÆ°á»›c 4: Kiá»ƒm tra Producer

```powershell
docker logs crypto-producer --tail 20
```

Káº¿t quáº£ mong Ä‘á»£i:
```
âœ… BTC-USD      Price: $   86,865.54
âœ… ETH-USD      Price: $    2,833.05
âœ… SOL-USD      Price: $      130.35
âœ… ADA-USD      Price: $        0.41
âœ… DOGE-USD     Price: $        0.15
ğŸ“Š Summary: 5/5 pairs sent successfully
```

### BÆ°á»›c 5: Verify dá»¯ liá»‡u trong Database

```powershell
docker exec postgres-db psql -U user -d crypto_data -c "SELECT 'Spark' as engine, COUNT(*) FROM crypto_prices_realtime UNION ALL SELECT 'Flink' as engine, COUNT(*) FROM crypto_prices_flink;"
```

Káº¿t quáº£ sau vÃ i phÃºt:
```
 engine | count
--------+-------
 Spark  |   75+
 Flink  |   50+
```

### BÆ°á»›c 6: Cháº¡y Performance Test

```powershell
.\compare_latency.ps1
```

**Káº¿t quáº£ mong Ä‘á»£i:** Flink nhanh hÆ¡n Spark **3-5 láº§n**.

### BÆ°á»›c 7: Truy cáº­p Dashboards

- **Airflow UI:** http://localhost:8080 (admin/admin)
- **Spark Master UI:** http://localhost:8081
- **Flink JobManager UI:** http://localhost:8082

---

## ğŸ“Š Apache Flink - Giá»›i thiá»‡u chi tiáº¿t

### Kiáº¿n trÃºc Apache Flink

Apache Flink lÃ  má»™t **distributed stream processing framework** Ä‘Æ°á»£c thiáº¿t káº¿ cho xá»­ lÃ½ real-time data vá»›i latency cá»±c tháº¥p.

#### Kiáº¿n trÃºc cá»‘t lÃµi

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    APACHE FLINK ARCHITECTURE                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[1] CLIENT LAYER
    Flink Application (Python/Java/Scala)
         â”‚
         â”œâ”€ DataStream API (imperative)
         â”œâ”€ Table API (declarative)
         â””â”€ SQL API (declarative)
         â”‚
         â–¼ Submit Job
         
[2] CONTROL PLANE
    JobManager (Master)
         â”‚
         â”œâ”€ JobGraph â†’ ExecutionGraph
         â”œâ”€ Resource Management
         â”œâ”€ Checkpoint Coordination
         â””â”€ Task Scheduling
         â”‚
         â–¼ Distribute Tasks
         
[3] DATA PLANE
    TaskManager 1       TaskManager 2       TaskManager 3
    â”œâ”€ Task Slot 1      â”œâ”€ Task Slot 1      â”œâ”€ Task Slot 1
    â”œâ”€ Task Slot 2      â”œâ”€ Task Slot 2      â”œâ”€ Task Slot 2
    â””â”€ Task Slot 3      â””â”€ Task Slot 3      â””â”€ Task Slot 3
         â”‚                   â”‚                   â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
[4] STATE MANAGEMENT
    State Backend (RocksDB / Heap)
         â”‚
         â”œâ”€ Keyed State (per key)
         â”œâ”€ Operator State (per parallel instance)
         â””â”€ Checkpoints (distributed snapshots)
```

#### Core Components

| Component | Vai trÃ² | Sá»‘ lÆ°á»£ng | Docker Service |
|-----------|---------|----------|----------------|
| **JobManager** | Master node, orchestration | 1 | flink-jobmanager |
| **TaskManager** | Worker node, execute tasks | 1+ | flink-taskmanager |
| **Task Slot** | Thread unit for parallelism | N Ã— TaskManager | Configured in env |
| **State Backend** | Persistent storage cho state | 1 (shared) | RocksDB/Heap |

### Æ¯u Ä‘iá»ƒm vÃ  NhÆ°á»£c Ä‘iá»ƒm

#### Æ¯u Ä‘iá»ƒm

| Æ¯u Ä‘iá»ƒm | MÃ´ táº£ | Use Case |
|---------|-------|----------|
| **Low Latency** | Xá»­ lÃ½ sub-second latency | Real-time fraud detection, HFT trading |
| **High Throughput** | Millions events/second | IoT data ingestion, log processing |
| **Exactly-Once** | Strong consistency guarantees | Financial transactions, billing systems |
| **Stateful Processing** | Built-in state management | Session analytics, pattern detection |
| **Event Time Processing** | Handle out-of-order events | Time-series analytics, late data handling |
| **Flexible Deployment** | Standalone, YARN, K8s, Mesos | Cloud-native or on-premise |
| **SQL Support** | Table API & SQL for streaming | Business analysts, rapid development |
| **Savepoints** | Version control for streaming apps | A/B testing, rolling updates |

#### NhÆ°á»£c Ä‘iá»ƒm

| NhÆ°á»£c Ä‘iá»ƒm | MÃ´ táº£ | Mitigation |
|-----------|-------|------------|
| **Steep Learning Curve** | Concepts phá»©c táº¡p (watermarks, state, checkpoints) | Báº¯t Ä‘áº§u vá»›i Table API trÆ°á»›c DataStream API |
| **Memory Intensive** | State backend cáº§n nhiá»u RAM | DÃ¹ng RocksDB cho large state, tune memory configs |
| **Operational Complexity** | Cáº§n monitoring checkpoint lag, backpressure | DÃ¹ng Flink Dashboard + Prometheus metrics |
| **Limited ML Support** | KhÃ´ng cÃ³ ML library nhÆ° Spark MLlib | TÃ­ch há»£p vá»›i TensorFlow, PyTorch riÃªng |
| **Smaller Ecosystem** | Ãt connectors hÆ¡n Spark | Community Ä‘ang phÃ¡t triá»ƒn nhanh |
| **Debugging Challenges** | Distributed debugging khÃ³ | DÃ¹ng local mode + extensive logging |

---

## âš–ï¸ So sÃ¡nh Apache Spark vs Apache Flink

### Kiáº¿n trÃºc xá»­ lÃ½

| TiÃªu chÃ­ | Apache Spark Structured Streaming | Apache Flink |
|----------|-----------------------------------|--------------|
| **Processing Model** | Micro-batch (15 giÃ¢y/batch) | True streaming (event-by-event) |
| **Core Abstraction** | RDD â†’ DataFrame/Dataset | DataStream â†’ Table |
| **State Management** | External state stores (HDFS, S3) | Built-in managed state (RocksDB) |
| **Latency** | Seconds (batch interval) | Milliseconds (event-driven) |
| **Throughput** | Excellent for large batches | Excellent for continuous streams |
| **Memory Model** | In-memory caching for speed | Streaming pipelined execution |
| **Fault Tolerance** | RDD lineage + checkpointing | Distributed snapshots (Chandy-Lamport) |

### API Comparison

**Spark Structured Streaming:**
```python
# Declarative API vá»›i DataFrame
from pyspark.sql import SparkSession
from pyspark.sql.functions import from_json, col

spark = SparkSession.builder.appName("CryptoStream").getOrCreate()

# Read stream
df = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "kafka:9092") \
    .option("subscribe", "crypto_prices") \
    .load()

# Transform (batch-like operations)
crypto_df = df.selectExpr("CAST(value AS STRING)") \
    .select(from_json(col("value"), schema).alias("data")) \
    .select("data.*")

# Write stream vá»›i trigger interval
query = crypto_df.writeStream \
    .outputMode("complete") \
    .trigger(processingTime="15 seconds") \
    .format("console") \
    .start()
```

**Flink Table API + SQL:**
```python
# DDL-style table creation
from pyflink.table import StreamTableEnvironment

table_env = StreamTableEnvironment.create(env)

# Kafka source
table_env.execute_sql("""
    CREATE TABLE crypto_source (
        symbol STRING,
        price DOUBLE,
        `timestamp` BIGINT,
        WATERMARK FOR event_time AS TO_TIMESTAMP_LTZ(`timestamp`, 3)
    ) WITH (
        'connector' = 'kafka',
        'topic' = 'crypto_prices',
        'properties.bootstrap.servers' = 'kafka:9092',
        'format' = 'json',
        'scan.startup.mode' = 'latest-offset'
    )
""")

# JDBC sink
table_env.execute_sql("""
    CREATE TABLE crypto_sink (
        symbol STRING,
        price DOUBLE,
        `user` STRING,
        `timestamp` BIGINT
    ) WITH (
        'connector' = 'jdbc',
        'url' = 'jdbc:postgresql://postgres-db:5432/crypto_data',
        'table-name' = 'crypto_prices_flink',
        'username' = 'user',
        'password' = 'password'
    )
""")

# Streaming query
table_env.execute_sql("INSERT INTO crypto_sink SELECT * FROM crypto_source")
```

### Performance Comparison

| Metric | Spark (Micro-batch 15s) | Flink (True Streaming) |
|--------|-------------------------|------------------------|
| **Latency** | 8-9 giÃ¢y | 1-3 giÃ¢y |
| **Throughput** | ~27 records/minute | ~26 records/minute |
| **Memory Usage** | 2-4 GB (executor heap) | 1-3 GB (task manager) |
| **CPU Usage** | Spiky (batch processing) | Smooth (continuous) |
| **Exactly-Once** | âœ… Vá»›i foreachBatch | âœ… Native support |
| **Late Data Handling** | âš ï¸ Limited watermark support | âœ… Advanced watermark strategies |

### Time Semantics

**Spark:**
```python
# Processing time (khi data Ä‘áº¿n Spark)
df.writeStream \
    .trigger(processingTime="15 seconds") \
    .start()

# Event time (limited support)
df.withWatermark("timestamp", "10 minutes")
```

**Flink:**
```python
# Event time vá»›i watermark strategy
from pyflink.common.watermark_strategy import WatermarkStrategy
from pyflink.common.time import Duration

strategy = WatermarkStrategy \
    .for_bounded_out_of_orderness(Duration.of_seconds(5)) \
    .with_timestamp_assigner(lambda event, ts: event['timestamp'])

stream.assign_timestamps_and_watermarks(strategy)
```

### State Management

| Feature | Spark | Flink |
|---------|-------|-------|
| **State Store** | External (HDFS/S3) | Embedded (RocksDB/Memory) |
| **State Size** | Limited by batch size | Unlimited (RocksDB disk) |
| **State Access** | Batch-based | Continuous access |
| **Checkpointing** | Incremental (Delta files) | Asynchronous barriers |
| **Recovery Time** | Minutes (batch replay) | Seconds (state restore) |

### Windowing Capabilities

**Spark (Limited):**
```python
# Fixed windows only
df.groupBy(
    window("timestamp", "5 minutes")
).count()
```

**Flink (Comprehensive):**
```python
# Tumbling Window
stream.key_by(...).window(TumblingEventTimeWindows.of(Time.minutes(5)))

# Sliding Window
stream.key_by(...).window(SlidingEventTimeWindows.of(
    Time.minutes(10),  # size
    Time.minutes(5)    # slide
))

# Session Window (activity gap-based)
stream.key_by(...).window(EventTimeSessionWindows.with_gap(Time.minutes(30)))

# Global Window vá»›i custom triggers
stream.key_by(...).window(GlobalWindows.create()).trigger(...)
```

---

## ğŸ”§ Äiá»u chá»‰nh tham sá»‘ Flink

### Cáº¥u hÃ¬nh trong docker-compose.yml

```yaml
flink-jobmanager:
  image: flink:1.18.0-scala_2.12-java11
  environment:
    - |
      FLINK_PROPERTIES=
      # === PARALLELISM & SLOTS ===
      taskmanager.numberOfTaskSlots: 4           # Sá»‘ task slots má»—i TaskManager
      parallelism.default: 2                     # Parallelism máº·c Ä‘á»‹nh
      
      # === MEMORY CONFIGURATION ===
      taskmanager.memory.process.size: 2048m     # Tá»•ng memory cho TaskManager
      taskmanager.memory.flink.size: 1536m       # Flink managed memory
      
      # === CHECKPOINT SETTINGS ===
      execution.checkpointing.interval: 60000    # Checkpoint má»—i 60 giÃ¢y
      execution.checkpointing.mode: EXACTLY_ONCE # At-least-once hoáº·c exactly-once
      
      # === STATE BACKEND ===
      state.backend: rocksdb                     # rocksdb hoáº·c filesystem
      state.checkpoints.dir: file:///tmp/flink-checkpoints
```

### Performance Tuning Parameters

**1. Parallelism (Äá»™ song song)**
```python
env = StreamExecutionEnvironment.get_execution_environment()

# Set global parallelism
env.set_parallelism(4)

# Set per-operator parallelism
stream.map(my_function).set_parallelism(8)
```

**NguyÃªn táº¯c:** `parallelism = sá»‘ TaskManager Ã— sá»‘ slots per TaskManager`  
**Demo nÃ y:** 3 Kafka partitions â†’ parallelism=2 hoáº·c 3

**2. Checkpointing (Fault Tolerance)**
```python
# Enable checkpointing
env.enable_checkpointing(60000)  # 60 seconds

# Checkpoint configuration
checkpoint_config = env.get_checkpoint_config()
checkpoint_config.set_checkpointing_mode(CheckpointingMode.EXACTLY_ONCE)
checkpoint_config.set_checkpoint_timeout(600000)  # 10 min timeout
```

**3. State Backend Selection**

| State Backend | Use Case | Max Size | Performance |
|---------------|----------|----------|-------------|
| **HashMap** | Small state (<100MB) | Limited by heap | Very fast |
| **RocksDB** | Large state (GBs-TBs) | Disk-bounded | Moderate (disk I/O) |

**4. Buffer Timeout (Latency Tuning)**
```python
env.set_buffer_timeout(100)  # milliseconds
```

| Buffer Timeout | Latency | Throughput | Use Case |
|----------------|---------|------------|----------|
| 0ms | Lowest | Lowest | Ultra-low latency apps |
| 100ms | Low | High | Balanced (recommended) |
| -1 (disabled) | Highest | Highest | Batch-like processing |

**5. Kafka Consumer Configuration**
```python
table_env.execute_sql("""
    CREATE TABLE crypto_source (...) WITH (
        'connector' = 'kafka',
        'properties.group.id' = 'flink-crypto-consumer',
        'scan.startup.mode' = 'latest-offset',
        'properties.fetch.min.bytes' = '1024',
        'properties.max.partition.fetch.bytes' = '1048576'
    )
""")
```

**6. JDBC Sink Tuning**
```python
table_env.execute_sql("""
    CREATE TABLE crypto_sink (...) WITH (
        'connector' = 'jdbc',
        'sink.buffer-flush.max-rows' = '100',         # Batch size
        'sink.buffer-flush.interval' = '1s',          # Flush interval
        'sink.max-retries' = '3',                     # Retry on failure
        'sink.parallelism' = '2'                      # Writer parallelism
    )
""")
```

---

## ğŸ“ˆ Performance Verification - Báº±ng chá»©ng Flink nhanh hÆ¡n

### Cháº¡y Performance Test

```powershell
.\compare_latency.ps1
```

Script nÃ y cháº¡y 4 tests Ä‘á»ƒ Ä‘o vÃ  so sÃ¡nh hiá»‡u suáº¥t giá»¯a Spark vÃ  Flink.

### Test 1: Average Latency

**Äo latency trung bÃ¬nh trong 5 phÃºt gáº§n Ä‘Ã¢y:**

```sql
SELECT 
    engine,
    AVG(processed_at_timestamp - producer_timestamp) as avg_latency_sec,
    COUNT(*) as sample_size
FROM (Spark table UNION Flink table)
WHERE processed_at > NOW() - INTERVAL '5 minutes';
```

**Káº¿t quáº£:**
```
 engine | avg_latency_sec | sample_size 
--------+-----------------+-------------
 Spark  |            8.83 |         120
 Flink  |            2.23 |         125
```

**PhÃ¢n tÃ­ch:**
- âœ… **Flink nhanh hÆ¡n 3.96x** (8.83s vs 2.23s)
- Spark: 8-9 giÃ¢y latency do micro-batch processing
- Flink: 2-3 giÃ¢y latency nhá» event-driven architecture

### Test 2: Latest Records Latency

**5 records má»›i nháº¥t tá»« má»—i engine:**

**Spark:**
```
  symbol  | latency_sec | db_time  
----------+-------------+----------
 DOGE-USD |           7 | 09:09:00
 ADA-USD  |           7 | 09:09:00
 SOL-USD  |           7 | 09:09:00
 ETH-USD  |           7 | 09:09:00
 BTC-USD  |           7 | 09:09:00
```

**Flink:**
```
  symbol  | latency_sec | db_time  
----------+-------------+----------
 DOGE-USD |           4 | 09:09:08
 ADA-USD  |           3 | 09:09:07
 SOL-USD  |           2 | 09:09:06
 ETH-USD  |           1 | 09:09:05
 BTC-USD  |           1 | 09:09:05
```

**PhÃ¢n tÃ­ch:**
- Spark: Táº¥t cáº£ records **cÃ¹ng latency (7s)** vÃ¬ batch processing
- Flink: Latency **khÃ¡c nhau (1-4s)** vÃ¬ xá»­ lÃ½ tá»«ng event
- âœ… **Flink nhanh hÆ¡n 5-7x**

### Test 3: Throughput Comparison

```
 engine |     records_per_min     
--------+-------------------------
 Spark  | 26.67
 Flink  | 26.11
```

**PhÃ¢n tÃ­ch:** âœ… **Throughput tÆ°Æ¡ng Ä‘Æ°Æ¡ng** (~26 records/min)

### Test 4: Data Freshness

```
 engine | time_since_last_write 
--------+-----------------------
 Spark  | 00:00:15.77
 Flink  | 00:00:06.91
```

**PhÃ¢n tÃ­ch:**
- Spark: Data cÅ© hÆ¡n **15.77 giÃ¢y**
- Flink: Data chá»‰ cÅ© **6.91 giÃ¢y**
- âœ… **Flink data má»›i hÆ¡n 2.3x**

### Giáº£i thÃ­ch táº¡i sao Flink nhanh hÆ¡n

#### Spark Micro-batch Processing

```
Timeline:
00:00  Producer sends â†’ Kafka
00:00  â”œâ”€ Message arrives in Kafka
00:00  â”œâ”€ Spark: Waiting for trigger (15s interval)
00:15  â””â”€ Trigger! Read all messages from last 15s
00:16      â”œâ”€ Parse JSON
00:17      â”œâ”€ Transform data
00:18      â””â”€ Write batch to PostgreSQL
       
Total Latency: 15-18 seconds
```

**NguyÃªn nhÃ¢n cháº­m:**
- â±ï¸ **Trigger Interval = 15 giÃ¢y:** Pháº£i Ä‘á»£i Ä‘á»§ thá»i gian má»›i xá»­ lÃ½
- ğŸ“¦ **Batch Processing:** Táº¥t cáº£ messages trong 15s Ä‘Æ°á»£c xá»­ lÃ½ cÃ¹ng lÃºc
- **Minimum Latency = Trigger Interval**

#### Flink Event-Driven Processing

```
Timeline:
00:00  Producer sends â†’ Kafka
00:00  â”œâ”€ Message arrives in Kafka
00:01  â”œâ”€ Flink reads event immediately
00:01  â”œâ”€ Parse JSON (in-flight)
00:02  â”œâ”€ Transform data (in-flight)
00:02  â””â”€ Write to PostgreSQL immediately

Total Latency: 1-3 seconds
```

**NguyÃªn nhÃ¢n nhanh:**
- âš¡ **Event-Driven:** Xá»­ lÃ½ ngay khi message Ä‘áº¿n
- ğŸ”„ **Pipelined Execution:** Parse â†’ Transform â†’ Write song song
- ğŸ’¨ **No Waiting:** KhÃ´ng cÃ³ trigger interval

### Báº£ng tÃ³m táº¯t Performance

| Metric | Spark | Flink | Winner |
|--------|-------|-------|--------|
| **Avg Latency** | 8.83s | 2.23s | âœ… Flink (3.96x) |
| **Min Latency** | 15s | 1s | âœ… Flink (15x) |
| **Throughput** | 26.67 rec/min | 26.11 rec/min | âš–ï¸ Equal |
| **Data Freshness** | 15.77s old | 6.91s old | âœ… Flink (2.3x) |

---

## ğŸ¯ Káº¿t luáº­n & Lá»±a chá»n

### Khi nÃ o dÃ¹ng Flink?

âœ… **Real-time dashboards:** Cáº§n update < 5 giÃ¢y  
âœ… **Fraud detection:** PhÃ¡t hiá»‡n gian láº­n ngay láº­p tá»©c  
âœ… **Live monitoring:** GiÃ¡m sÃ¡t há»‡ thá»‘ng real-time  
âœ… **Trading systems:** High-frequency trading  
âœ… **IoT streaming:** Sensor data processing  
âœ… **Alerting systems:** Gá»­i alert trong vÃ i giÃ¢y  

### Khi nÃ o dÃ¹ng Spark?

âœ… **ETL pipelines:** Batch + streaming trong cÃ¹ng code  
âœ… **Data warehousing:** Load data má»—i 15-30 phÃºt  
âœ… **Machine Learning:** Training models trÃªn streaming data  
âœ… **Report generation:** Táº¡o bÃ¡o cÃ¡o Ä‘á»‹nh ká»³  
âœ… **Large batch jobs:** Xá»­ lÃ½ terabytes data  

### Báº£ng lá»±a chá»n

| TiÃªu chÃ­ | Spark | Flink | Chá»n gÃ¬? |
|----------|-------|-------|----------|
| **Latency requirement** | 10-30s OK | < 5s cáº§n | Flink cho real-time |
| **Data volume** | Terabytes | Gigabytes | Spark cho big batch |
| **Team experience** | Spark ecosystem | Flink learning curve | Spark dá»… hÆ¡n |
| **Use case** | Analytics, ML | Monitoring, alerting | Depends |
| **Cost** | Lower (batch efficient) | Higher (always running) | Spark ráº» hÆ¡n |

---

## ğŸ›‘ Stop System

```powershell
# Stop all containers
docker-compose down

# Stop and remove volumes (clean slate)
docker-compose down -v
```

---

## ğŸ“ Káº¿t luáº­n tá»•ng quan

**Apache Spark Structured Streaming** vÃ  **Apache Flink** Ä‘á»u lÃ  cÃ´ng cá»¥ máº¡nh máº½ cho xá»­ lÃ½ streaming:

- **Spark**: PhÃ¹ há»£p cho batch + streaming, latency 8-15 giÃ¢y, dá»… há»c náº¿u Ä‘Ã£ biáº¿t Spark ecosystem
- **Flink**: Latency tháº¥p 1-3 giÃ¢y, event-driven, phá»©c táº¡p hÆ¡n nhÆ°ng máº¡nh máº½ cho real-time analytics

**Káº¿t quáº£ thá»±c táº¿ tá»« demo nÃ y:**
- Producer gá»­i 5 crypto pairs má»—i 10 giÃ¢y
- Spark xá»­ lÃ½ theo batch 15 giÃ¢y â†’ **latency 8.83s**
- Flink xá»­ lÃ½ real-time tá»«ng event â†’ **latency 2.23s**
- Cáº£ hai Ä‘á»u ghi vÃ o PostgreSQL Ä‘á»ƒ so sÃ¡nh side-by-side

**Báº±ng chá»©ng cá»¥ thá»ƒ:** Cháº¡y `.\compare_latency.ps1` Ä‘á»ƒ xem Flink nhanh hÆ¡n Spark **3.96 láº§n**.

**Lá»±a chá»n phá»¥ thuá»™c vÃ o:** YÃªu cáº§u latency, data volume, team experience, vÃ  budget.

