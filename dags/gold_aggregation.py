from datetime import datetime, timedelta
from airflow import DAG
from airflow.providers.postgres.operators.postgres import PostgresOperator
from airflow.providers.postgres.hooks.postgres import PostgresHook
from airflow.operators.python import PythonOperator

default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2025, 11, 8),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 2,
    'retry_delay': timedelta(minutes=2),
}

dag = DAG(
    'gold_hourly_aggregation',
    default_args=default_args,
    description='Aggregate Silver data into Gold hourly metrics',
    schedule_interval=timedelta(minutes=10),  # Ch·∫°y m·ªói 10 ph√∫t
    catchup=False,
    tags=['gold', 'aggregation', 'analytics'],
)

def calculate_hourly_metrics(**context):
    """
    ƒê·ªçc d·ªØ li·ªáu t·ª´ Silver layer (crypto_prices_realtime)
    T√≠nh to√°n metrics theo gi·ªù v√† ghi v√†o Gold layer (gold_hourly_metrics)
    """
    
    # K·∫øt n·ªëi ƒë·∫øn PostgreSQL
    pg_hook = PostgresHook(postgres_conn_id='postgres_crypto')
    
    # SQL ƒë·ªÉ t√≠nh to√°n metrics theo gi·ªù
    aggregation_sql = """
    WITH hourly_data AS (
        SELECT 
            date_trunc('hour', processed_at) as hour_timestamp,
            symbol,
            AVG(price) as avg_price,
            MIN(price) as min_price,
            MAX(price) as max_price,
            SUM(volume_24h) as total_volume,
            AVG(volume_24h) as avg_volume,
            COUNT(*) as record_count
        FROM crypto_prices_realtime
        WHERE processed_at >= NOW() - INTERVAL '2 hours'  -- Ch·ªâ x·ª≠ l√Ω 2 gi·ªù g·∫ßn nh·∫•t
        GROUP BY date_trunc('hour', processed_at), symbol
    ),
    previous_hour AS (
        SELECT 
            hour_timestamp + INTERVAL '1 hour' as next_hour,
            symbol,
            avg_price as prev_avg_price
        FROM gold_hourly_metrics
        WHERE hour_timestamp >= NOW() - INTERVAL '3 hours'
    )
    INSERT INTO gold_hourly_metrics (
        hour_timestamp,
        symbol,
        avg_price,
        min_price,
        max_price,
        total_volume,
        avg_volume,
        price_change,
        price_change_percent,
        record_count
    )
    SELECT 
        h.hour_timestamp,
        h.symbol,
        h.avg_price,
        h.min_price,
        h.max_price,
        h.total_volume,
        h.avg_volume,
        COALESCE(h.avg_price - p.prev_avg_price, 0) as price_change,
        COALESCE(
            CASE 
                WHEN p.prev_avg_price > 0 
                THEN ((h.avg_price - p.prev_avg_price) / p.prev_avg_price) * 100
                ELSE 0
            END, 
            0
        ) as price_change_percent,
        h.record_count
    FROM hourly_data h
    LEFT JOIN previous_hour p 
        ON h.hour_timestamp = p.next_hour 
        AND h.symbol = p.symbol
    ON CONFLICT (hour_timestamp) 
    DO UPDATE SET
        avg_price = EXCLUDED.avg_price,
        min_price = EXCLUDED.min_price,
        max_price = EXCLUDED.max_price,
        price_change = EXCLUDED.price_change,
        price_change_percent = EXCLUDED.price_change_percent,
        record_count = EXCLUDED.record_count,
        created_at = CURRENT_TIMESTAMP;
    """
    
    try:
        # Th·ª±c thi SQL
        pg_hook.run(aggregation_sql)
        
        # L·∫•y s·ªë l∆∞·ª£ng records ƒë√£ x·ª≠ l√Ω
        result = pg_hook.get_first(
            "SELECT COUNT(*) FROM gold_hourly_metrics WHERE created_at >= NOW() - INTERVAL '1 minute';"
        )
        
        count = result[0] if result else 0
        print(f"‚úÖ ƒê√£ t·ªïng h·ª£p th√†nh c√¥ng! {count} hourly metrics ƒë√£ ƒë∆∞·ª£c c·∫≠p nh·∫≠t/t·∫°o m·ªõi.")
        
        # Log sample data
        sample = pg_hook.get_records(
            "SELECT * FROM gold_hourly_metrics ORDER BY hour_timestamp DESC LIMIT 3;"
        )
        print(f"üìä Sample data:\n{sample}")
        
        return count
        
    except Exception as e:
        print(f"‚ùå L·ªói khi t·ªïng h·ª£p d·ªØ li·ªáu: {str(e)}")
        raise

def cleanup_old_data(**context):
    """
    X√≥a d·ªØ li·ªáu c≈© h∆°n 7 ng√†y ƒë·ªÉ t·ªëi ∆∞u storage
    """
    pg_hook = PostgresHook(postgres_conn_id='postgres_crypto')
    
    cleanup_sql = """
    DELETE FROM gold_hourly_metrics 
    WHERE hour_timestamp < NOW() - INTERVAL '7 days';
    """
    
    try:
        result = pg_hook.run(cleanup_sql)
        print(f"‚úÖ ƒê√£ x√≥a d·ªØ li·ªáu c≈© th√†nh c√¥ng!")
        return result
    except Exception as e:
        print(f"‚ö†Ô∏è L·ªói khi cleanup: {str(e)}")
        # Kh√¥ng raise error v√¨ cleanup kh√¥ng quan tr·ªçng b·∫±ng aggregation

# Task 1: T√≠nh to√°n metrics t·ª´ Silver ‚Üí Gold
aggregate_task = PythonOperator(
    task_id='aggregate_hourly_metrics',
    python_callable=calculate_hourly_metrics,
    provide_context=True,
    dag=dag,
)

# Task 2: Cleanup d·ªØ li·ªáu c≈© (optional)
cleanup_task = PythonOperator(
    task_id='cleanup_old_metrics',
    python_callable=cleanup_old_data,
    provide_context=True,
    dag=dag,
)

# Task 3: Validate d·ªØ li·ªáu Gold
validate_sql = """
SELECT 
    COUNT(*) as total_hours,
    MAX(hour_timestamp) as latest_hour,
    SUM(record_count) as total_records_processed
FROM gold_hourly_metrics
WHERE hour_timestamp >= NOW() - INTERVAL '24 hours';
"""

validate_task = PostgresOperator(
    task_id='validate_gold_data',
    postgres_conn_id='postgres_crypto',
    sql=validate_sql,
    dag=dag,
)

# ƒê·ªãnh nghƒ©a th·ª© t·ª± th·ª±c thi
aggregate_task >> validate_task >> cleanup_task
